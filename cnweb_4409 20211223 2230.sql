--
-- Script was generated by Devart dbForge Studio 2019 for MySQL, Version 8.2.23.0
-- Product home page: http://www.devart.com/dbforge/mysql/studio
-- Script date 12/23/2021 10:30:05 PM
-- Server version: 5.5.5-10.2.39-MariaDB
-- Client version: 4.1
--

-- 
-- Disable foreign keys
-- 
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;

-- 
-- Set SQL mode
-- 
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;

-- 
-- Set character set the client will use to send SQL statements to the server
--
SET NAMES 'utf8';

--
-- Set default database
--
USE cnweb_4409;

--
-- Drop table `questiontag`
--
DROP TABLE IF EXISTS questiontag;

--
-- Drop table `usertag`
--
DROP TABLE IF EXISTS usertag;

--
-- Drop table `tags`
--
DROP TABLE IF EXISTS tags;

--
-- Drop table `useranswer`
--
DROP TABLE IF EXISTS useranswer;

--
-- Drop table `answer`
--
DROP TABLE IF EXISTS answer;

--
-- Drop table `userquestion`
--
DROP TABLE IF EXISTS userquestion;

--
-- Drop table `question`
--
DROP TABLE IF EXISTS question;

--
-- Drop table `user`
--
DROP TABLE IF EXISTS user;

--
-- Set default database
--
USE cnweb_4409;

--
-- Create table `user`
--
CREATE TABLE user (
  Id char(36) NOT NULL,
  Username varchar(255) DEFAULT NULL,
  Password varchar(255) DEFAULT NULL,
  Description varchar(255) DEFAULT NULL,
  Favorite varchar(255) DEFAULT NULL,
  Star int(11) DEFAULT NULL,
  Name varchar(50) DEFAULT NULL,
  PRIMARY KEY (Id)
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 321,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create index `UK_user` on table `user`
--
ALTER TABLE user
ADD UNIQUE INDEX UK_user (Name, Id);

--
-- Create index `UK_user_Name` on table `user`
--
ALTER TABLE user
ADD UNIQUE INDEX UK_user_Name (Name);

--
-- Create table `question`
--
CREATE TABLE question (
  Id char(36) NOT NULL,
  Title varchar(255) DEFAULT NULL,
  Content varchar(5000) DEFAULT NULL,
  CreateTime datetime DEFAULT NULL,
  ModifyTime datetime DEFAULT NULL,
  UserId char(36) NOT NULL,
  View int(11) DEFAULT NULL,
  PRIMARY KEY (Id)
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 327,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create foreign key
--
ALTER TABLE question
ADD CONSTRAINT FK_question_UserId FOREIGN KEY (UserId)
REFERENCES user (Id) ON DELETE NO ACTION;

--
-- Create table `userquestion`
--
CREATE TABLE userquestion (
  UserId char(36) DEFAULT NULL,
  QuestionId char(36) DEFAULT NULL,
  Status int(11) NOT NULL
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 160,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create foreign key
--
ALTER TABLE userquestion
ADD CONSTRAINT FK_userquestion_QuestionId FOREIGN KEY (QuestionId)
REFERENCES question (Id) ON DELETE NO ACTION;

--
-- Create foreign key
--
ALTER TABLE userquestion
ADD CONSTRAINT FK_userquestion_UserId FOREIGN KEY (UserId)
REFERENCES user (Id) ON DELETE NO ACTION;

--
-- Create table `answer`
--
CREATE TABLE answer (
  Id char(36) NOT NULL,
  Content varchar(5000) DEFAULT NULL,
  CreateTime datetime DEFAULT NULL,
  ModifyTime datetime DEFAULT NULL,
  UserId char(36) DEFAULT NULL,
  QuestionId char(36) DEFAULT NULL,
  PRIMARY KEY (Id)
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 327,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create foreign key
--
ALTER TABLE answer
ADD CONSTRAINT FK_answer_QusetionId FOREIGN KEY (QuestionId)
REFERENCES question (Id) ON DELETE NO ACTION;

--
-- Create foreign key
--
ALTER TABLE answer
ADD CONSTRAINT FK_answer_UserId FOREIGN KEY (UserId)
REFERENCES user (Id) ON DELETE NO ACTION;

--
-- Create table `useranswer`
--
CREATE TABLE useranswer (
  UserId char(36) DEFAULT NULL,
  AnswerId char(36) DEFAULT NULL,
  Status int(11) NOT NULL
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 292,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create foreign key
--
ALTER TABLE useranswer
ADD CONSTRAINT FK_useranswer_AnswerId FOREIGN KEY (AnswerId)
REFERENCES answer (Id) ON DELETE NO ACTION;

--
-- Create foreign key
--
ALTER TABLE useranswer
ADD CONSTRAINT FK_useranswer_UserId FOREIGN KEY (UserId)
REFERENCES user (Id) ON DELETE NO ACTION;

--
-- Create table `tags`
--
CREATE TABLE tags (
  Id char(36) NOT NULL,
  Name varchar(50) DEFAULT NULL,
  PRIMARY KEY (Id)
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 327,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create table `usertag`
--
CREATE TABLE usertag (
  UserId char(36) DEFAULT NULL,
  TagId char(36) DEFAULT NULL
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 327,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create foreign key
--
ALTER TABLE usertag
ADD CONSTRAINT FK_usertag_TagId FOREIGN KEY (TagId)
REFERENCES tags (Id) ON DELETE NO ACTION;

--
-- Create foreign key
--
ALTER TABLE usertag
ADD CONSTRAINT FK_usertag_UserId FOREIGN KEY (UserId)
REFERENCES user (Id) ON DELETE NO ACTION;

--
-- Create table `questiontag`
--
CREATE TABLE questiontag (
  QuestionId char(36) DEFAULT NULL,
  TagId char(36) DEFAULT NULL
)
ENGINE = INNODB,
AVG_ROW_LENGTH = 327,
CHARACTER SET utf8,
COLLATE utf8_general_ci;

--
-- Create foreign key
--
ALTER TABLE questiontag
ADD CONSTRAINT FK_questiontag_QuestionId FOREIGN KEY (QuestionId)
REFERENCES question (Id) ON DELETE NO ACTION;

--
-- Create foreign key
--
ALTER TABLE questiontag
ADD CONSTRAINT FK_questiontag_TagId FOREIGN KEY (TagId)
REFERENCES tags (Id) ON DELETE NO ACTION;

-- 
-- Dumping data for table user
--
INSERT INTO user VALUES
('11643064-22fb-7e14-1399-04b3a3d51ef2', 'pgatpfig_zooib@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Qui explicabo repellat. Minima quia magni. Ducimus sunt et. Sed vero et. Accusamus autem sit. Voluptatem quo dolorem; ut distinctio accusamus.', 'L2', 14, 'Travis2026'),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', 'Denice_Reich@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Eos enim error saepe et soluta perferendis. Unde molestiae provident qui possimus architecto deserunt.', 'GN5M92Y14454', 33, 'Adam1966'),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', 'kkd@gmail.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'ssss s saassass', '', 0, 'Tan Ngo'),
('15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', 'Broadway@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Qui sit error eveniet nulla tenetur corrupti. Dolore nihil voluptatem? Voluptatem fuga error. Consequatur illo natus! Corporis dolor minima. Sequi enim.', '93F438QC70C4H1R9TL8I42W3070O96QYHK3GIJ66CGMWI6IV5', 34, 'Quintin42'),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', 'nlwccgg0@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Eius rerum minus; ut molestias perferendis. Error iste sit. Asperiores iste illo; sunt qui error. Est architecto quas! Iste amet totam; voluptatem optio.', 'JMUR83F7D528ZJ3F21J99F5GBAI5VRZM88U4S9E5AMS', 45, 'Acevedo679'),
('15f49a4f-6117-2234-1544-cfd00e0a9b3b', 'HubertThomason@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Iste atque distinctio. Id nisi veritatis. Omnis error rerum; perspiciatis ut repellat. Aut labore deleniti. Ea tempora cumque. Magni et similique. Incidunt;', 'D1L7', 26, 'Felix1961'),
('1876fb44-59e4-7596-dcb8-94f47731937d', 'pspu1089@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Omnis eveniet quia. Sit sint sed. Aut ipsa aut. Hic necessitatibus minus? Placeat natus quas! Sit aperiam voluptas. Itaque omnis totam. Aut unde saepe.', NULL, 14, 'Abernathy2006'),
('18bce813-54a9-6649-f8c3-4d1f326d5ebd', 'Will593@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Ut itaque quaerat; autem delectus et. Ipsa aliquid voluptatem? Obcaecati dolores et. Deleniti facilis perferendis. Aliquid neque omnis? Aut voluptates!', 'BDS46B0V885FX1LA18TE50T70ZF695LQX3QMI6RB786GI637415Y62LZU21T3CNWW4509KXJLURV35964HFV8Y77E4803J725Q8Y4PYPLT20QXSA2N91IZ1WHTQ6FXY46B5088Z6', 12, 'Acevedo1950'),
('1beedfca-2f14-1d94-8865-390639088226', 'Clanton@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Quibusdam modi aliquid vitae sequi quaerat reprehenderit. Perspiciatis nemo vel error distinctio dolores cupiditate.', 'O5U3G7MASL3P0R5O8', 42, 'Hatley1953'),
('1e3c96c9-1471-23e0-8765-390639088226', 'Pritchett172@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Eum voluptatem aut laboriosam qui neque ipsa.', '0BIOIB6280KZE45I3A3G5M1M0QP1R9ZT14469O5H8DX9TX52535RJ1C1K14LMECNSCJ253Y67M860UPIZCKQ4KNHIYJ3MW17O1KDY5221LX8ZHH3QARB0BHQ3F7P1M30OV59RW095XB80OAH58469TKM1OZ99LT', 37, 'Abrams4'),
('1fee0bc4-513a-2c25-004e-8601bf3d85d8', 'Quincy_Angel@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Voluptatem ducimus accusamus. Nostrum tempore et! Qui dolorem et. Et incidunt accusantium; aut cumque unde. Aspernatur odit asperiores. Incidunt et quia. Ab!', '9', 28, 'Jesse1965'),
('2e41b1ec-1c05-1bc6-dab8-94f47731937d', 'Fuchs@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Rerum laudantium et nulla omnis hic aut. Eum facilis qui laboriosam voluptatem provident dolorem.', 'AS95I', 17, 'Meaghan12'),
('2ff554db-1bde-573a-701a-bb545470e80c', 'CaroleAcker@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Aspernatur animi eum enim quas. Iste aut aliquam sequi. Earum ut quisquam incidunt repellendus ab voluptatem.', 'XC1I258R3KFT1W3E1070D56M322V1', 45, 'Labbe2006'),
('30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', 'FelicitasBowden@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Sit sint consequuntur. Molestiae est iste. Sunt maiores ullam! Aut est tempora; saepe iste praesentium; voluptatem quis eligendi. Consequatur sint iste.', '0', 5, 'Beau73'),
('33c6f0f4-3591-2c8e-1244-cfd00e0a9b3b', 'EliasZPotts68@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Voluptatem velit sed; voluptatem laudantium consectetur. Cum praesentium magnam; accusantium sed nihil. Molestiae esse natus! Recusandae accusamus enim.', 'HY9N40N8W5N1C', 45, 'Elliot684'),
('33fad8ab-365a-2815-e078-cf6d4dd5c4a3', 'MargeretAlonzo295@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Ut unde voluptas. Ex necessitatibus culpa? Vitae officia iste. Aperiam alias voluptatem. Et reiciendis quod. Nostrum aliquid inventore. Dolores sapiente at!', 'V41922DH9GQ1E7MY2Z626HGP6L47H167J1O446SR', 11, 'Alfaro463'),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', 'Alba@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Culpa quas eveniet ipsa ducimus. Sed non aut nihil dolores est; dolorem ipsa nesciunt; non ex ut. Sunt consequatur repellat.', 'ZG8WX56HQOZEN51', 14, 'Aldridge2029'),
('3cc58118-77ec-59ed-711a-bb545470e80c', 'Chandler@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Eos perspiciatis sequi tenetur. Voluptatum ut eius dolorum ut sapiente magnam! Fugit ut et est modi cum rerum.', '4HM3T4V468CQ', 44, 'Purdy1990'),
('3d1c5555-3e10-56cf-2b8f-4d82f3a13455', 'Rhea@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Dolorum omnis unde. Est qui corporis. Porro eligendi et autem nulla ad. Quo totam et est blanditiis et dolorem.', 'ECU136667Z85LG1577945RO6880N2T4102Q186LR5709LM55H2WP96AK37BZ4JA9R7ZH789FU8SK8856MVT87P485ETQSN9UT2A3J4U6P34L2YN14J4W9DD73935PZK328D571', 28, 'Emmy64'),
('3d6207a1-1e88-4e30-f403-13a65cc92c97', 'Chang.Ferrell5@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Perferendis quas dicta dolores ad libero. Ut aut possimus. Et maxime qui; qui numquam temporibus. Dolorum rerum possimus. Expedita iste nulla...', 'WO35M8875KE319P1Z98E4JTG4O44RT497P6293M2D5XG197H62S3895EWJ', 37, 'Sylvester833'),
('412d134d-6bed-125f-1644-cfd00e0a9b3b', 'Allen_Abel42@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Ut exercitationem sunt. Est ea nemo; dolores ipsam et. Totam eius recusandae. Commodi dicta ut. Minus aut eaque; mollitia aut odit. Corporis ut praesentium.', '357YW9VR55J5F7T40787V6JIXGK', 45, 'Chery1961'),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', 'Leak@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Quis iusto enim. Consequatur beatae itaque? Minus dicta consequatur. Unde aut commodi. Error est iure. Recusandae repellat perferendis. Ullam aspernatur.', '5D4ZS8841Z2LK5LCURHZS89B55U8P22T00Z0EJ76H2340Y3Y89792D8184I97V0096I2BNN34917HCB8K77FEI4X18GEAJ5R08', 1, 'Domingo2029'),
('499e3b76-34a8-76c9-1599-04b3a3d51ef2', 'Abernathy797@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Assumenda ab enim. Sed aliquid et. Id ut officiis. Et vitae esse! Minus voluptate commodi. Aliquam qui aut! Aut tempore maiores; molestiae doloribus.', '6D02RRCDE142QX9YL52JLZSB93320U6NO4MI1O43FQUEJHO', 40, 'Minerva855'),
('4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', 'Ewing@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Sit id ab cum omnis fugiat omnis.', NULL, 14, 'Milliken2004'),
('4b635464-5ee2-2a80-f103-13a65cc92c97', 'EltonStanton941@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Animi autem ut; placeat error iure. Error eum placeat? Dolorum qui dolores. Eveniet omnis iusto? Qui ut alias! Est voluptas nam.', 'QQEL', 11, 'Abe9'),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', 'StuartZ.Kinder@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Et labore est. Minus ut et! Natus sunt ut? Quod voluptate nobis! Dignissimos perferendis a. Rerum eos nihil. Laborum et veritatis. Sed nemo rem; dolor sit.', 'OK2XVQX1ULF34570', 50, 'Lupita4'),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', 'Robbie_Abel@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Eum explicabo repellendus; nulla et nam? Natus velit enim. Minus voluptatem dolores. Iure sequi ipsa. Perspiciatis omnis autem. Tenetur sunt sed.', '7E0TDOQC2N1S62CCBG6DM7XT78SY1JYT4V2BR0TQELVEOLG97CZU8FE6CF71NJ81YEQ9', 17, 'Jerry175'),
('4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', 'Frame75@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Aut sed sint non minima tempore. Ea repellendus omnis sequi deleniti. Consequatur sunt quidem in suscipit aut molestiae. Iusto aut harum...', 'ZM8L', 11, 'Wendi434'),
('5ada22d3-3248-6596-6e1a-bb545470e80c', 'Davis372@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Eum sed esse voluptatem in sequi alias; eveniet dolor distinctio et iure aut? Necessitatibus ab perspiciatis. Illo sed in. Nihil architecto ullam.', '1T5EU3T85Z6LCH8MZF294709QF9QIOD5F47Z34V8965LEKDW556D3VK3D6NRXZK', 34, 'Margorie3'),
('5bc250e9-354c-43e2-a091-9e1ee725216c', 'hoangha@gmail.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'The Tan', '', 0, 'Minh Tung'),
('5bfe5648-65c2-7d1a-f203-13a65cc92c97', 'Rutherford@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Nulla enim maxime quam error consectetur debitis. Est tenetur iure. Nisi dolor eligendi. Deserunt repellendus consequuntur. Possimus maiores minus!', 'EB', 8, 'Malinda2004'),
('5e272913-6cff-509f-d8b8-94f47731937d', 'Acker@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Quia doloribus dolores. Error voluptatem error. Ea quos dolore. Corporis ut aut? Et illum cumque. Rerum sunt ut. Fugit id eum.', '80SR5RBK38B9425ORV707D43', 3, 'Burgess1955'),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', 'Michael96@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Repudiandae eligendi eum. Natus amet perferendis libero nesciunt quas. Impedit quibusdam voluptatem. Consectetur ullam recusandae. Dolorem omnis voluptatem.', 'EC5OK4W8P8O8B0V', 9, 'Trent226'),
('5ef3b4c9-75c9-482e-1699-04b3a3d51ef2', 'Geoffrey.IKnott9@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Qui et tenetur qui illum rem. Fugiat perspiciatis animi natus exercitationem cum voluptas. In ut nemo qui.', '2', 0, 'Mcgrath2019'),
('609d0ea1-772a-1b4a-8965-390639088226', 'LynneMackay629@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Est autem ab iste incidunt et eius.', 'KD38I2C1L0K4R516H2059828FRO7HDG757LA69YUFD3B058UM09728676K158B', 40, 'Taisha1983'),
('60e226cb-63fb-4363-fc4d-8601bf3d85d8', 'Asbury@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Qui perspiciatis odit. Modi et voluptatem ea. Quia consequatur inventore ut. Provident doloribus itaque aut mollitia qui quo!', NULL, 5, 'Cleary6'),
('6207a9a7-3bf9-7288-8565-390639088226', 'LuttrellZ@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Veniam nemo aliquid. Error natus eos officia qui dolorem sed! Perferendis sapiente nihil architecto nostrum cumque et.', '7589', 1, 'Alexandra2015'),
('621899dc-177b-65d2-dbb8-94f47731937d', 'Valentine.Q.Meeks@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Sit unde ratione. Harum eius hic. Ut esse cupiditate. Et quasi rerum. Ipsam accusantium facilis. Eum sed qui. Voluptas sed magnam.', '81677G', 22, 'Agatha2000'),
('68069d95-76b4-1dce-f503-13a65cc92c97', 'Clancy193@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Totam obcaecati inventore cum eum quia cupiditate. Maiores sunt cupiditate sed excepturi et aliquid.', NULL, 40, 'Angulo2000'),
('694c75bd-3b3d-2034-6f1a-bb545470e80c', 'Kelly.Pinckney@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Quidem non est. Modi quam minus. Eos natus assumenda? Maiores sint qui. Quia accusantium ut. Tempora facilis qui. Facilis iusto odio.', 'REUGL47QJ9LKH5UIWHXI05K622R53UNV867UOMS4577W0HBO3Q5T', 9, 'Branham885'),
('697be7ba-1738-6adf-298f-4d82f3a13455', 'ArdenAlcorn@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Nisi suscipit consequuntur autem eum ea blanditiis.', 'IK1735E152DP397ZD2AW0BG5MS3X9W85RNRWFF0O58F63SO1IIFOQNIBP18Q', 29, 'Swanson4'),
('6d81a322-63f0-7270-6d1a-bb545470e80c', 'mdcxmjf2@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Et cumque ipsam. Illo fugiat aut. Beatae mollitia temporibus. Dolore corporis perspiciatis. Corporis quasi ducimus. Saepe pariatur porro! Pariatur eum sit.', NULL, 13, 'Arsenault944'),
('6ed2e962-178c-712c-fd4d-8601bf3d85d8', 'TwannaAndrus@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Excepturi saepe omnis. Cumque error ut explicabo ut! Omnis vel quis dolor; autem optio obcaecati sed perspiciatis eaque fugiat. Qui non vel...', '0BTXU8PG', 48, 'Drummond36'),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', 'Bagwell@nowhere.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Deleniti ut natus impedit rerum et dolorem.', 'L', 31, 'Lemay2002'),
('6f2979d3-58ec-3673-f303-13a65cc92c97', 'AlessandraPruett@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Doloremque eum velit voluptatem. Magni enim ipsam aliquid quae laudantium doloribus! Tempora consequatur maiores corporis voluptatem quae consectetur.', '1146Z8MO43P51U433M8', 26, 'Bottoms5'),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', 'Johnathon_Sander@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Cumque aut ipsum sed nihil accusantium veritatis. Ab beatae ut quisquam rerum deleniti dolor!', 'IT0132179NS8Y6874', 4, 'Jovan494'),
('75de26bb-5a5c-467f-2c8f-4d82f3a13455', 'Thornburg757@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Dolor facere sit. Et ut voluptas id officiis ratione recusandae! Voluptas eos omnis. Totam laboriosam nesciunt. Omnis qui qui. Doloribus harum corporis!', 'Y6', 44, 'Perkins1963'),
('76598eb0-20b1-664e-8665-390639088226', 'RollerO1@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Ipsa dolorem labore. Quo laboriosam nisi. Fugit illum laborum; enim dolores fugit. Exercitationem et vero! Architecto quod perspiciatis. Rem debitis rerum.', 'AGCL6Z', 1, 'Adelina79'),
('76753dc8-4972-2ecd-2e8f-4d82f3a13455', 'Ahern16@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Necessitatibus ut voluptatem beatae hic autem voluptatem. Quibusdam officia similique enim quo eaque ut.', '76418Z649R1H88227JFZWBI', 20, 'Arndt91'),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', 'rmfz953@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Quaerat et omnis et quis. Nulla laudantium et blanditiis magni. Sed et tempora sed vitae vero; ducimus aliquid id. Omnis sunt repudiandae!', '1ZP9U3B3F1R8JR9K8X09X0623723033L629L3W63C26G8ZY369VZ66VZSF0012P5MI037QU262BR63R3H', 31, 'Stubblefield4'),
('7ac8b8cf-6a0d-6fa3-d9b8-94f47731937d', 'Violeta_Abel@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Architecto perspiciatis enim ratione itaque neque. Praesentium odio dignissimos? Ut veritatis vitae. Dolor maxime voluptatem totam sunt enim et.', '6HDI', 12, 'Scotty85'),
('7de0e887-6ace-4435-1344-cfd00e0a9b3b', 'Elliot.Cannon@example.com', 'C8-1A-16-0C-D7-6D-3C-3C-C1-45-00-F1-59-10-15-AF', 'Voluptatem quis eos nam laudantium debitis asperiores. Voluptatem vel quo nulla. Ea inventore adipisci magni natus corporis sunt!', 'G0KOVKE4291084RXV468ES1MEH776137UE5D2', 41, 'Brunilda8'),
('a60823fd-1a19-4726-bf64-0656b04a2bbb', 's', '47-BC-E5-C7-4F-58-9F-48-67-DB-D5-7E-9C-A9-F8-08', '', '', 0, 'a');

-- 
-- Dumping data for table question
--
INSERT INTO question VALUES
('042272c3-cfff-4dab-81ef-e90b29f3e52a', 'The Two Cultures: statistics vs. machine learning?', '<p>Last year, I read a blog post from <a href="http://anyall.org/">Brendan O''Connor</a> entitled <a href="http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/">"Statistics vs. Machine Learning, fight!"</a> that discussed some of the differences between the two fields.  <a href="http://andrewgelman.com/2008/12/machine_learnin/">Andrew Gelman responded favorably to this</a>:</p>\n\n<p>Simon Blomberg: </p>\n\n<blockquote>\n  <p>From R''s fortunes\n  package: To paraphrase provocatively,\n  ''machine learning is statistics minus\n  any checking of models and\n  assumptions''.\n  -- Brian D. Ripley (about the difference between machine learning\n  and statistics) useR! 2004, Vienna\n  (May 2004) :-) Season''s Greetings!</p>\n</blockquote>\n\n<p>Andrew Gelman:</p>\n\n<blockquote>\n  <p>In that case, maybe we should get rid\n  of checking of models and assumptions\n  more often. Then maybe we''d be able to\n  solve some of the problems that the\n  machine learning people can solve but\n  we can''t!</p>\n</blockquote>\n\n<p>There was also the <a href="http://projecteuclid.org/euclid.ss/1009213726"><strong>"Statistical Modeling: The Two Cultures"</strong> paper</a> by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the <em>predictive accuracy</em> of models.</p>\n\n<p>Has the statistics field changed over the last decade in response to these critiques?  Do the <em>two cultures</em> still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?</p>\n', '2021-12-15 00:28:56', '2021-12-15 00:28:56', '18bce813-54a9-6649-f8c3-4d1f326d5ebd', 177),
('11a8748f-3464-740c-28c2-579daad24557', 'Forecasting demographic census', '<p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\n\n<p>Some of the concerns:</p>\n\n<ul>\n<li>Census blocks vary in sizes as rural\nareas are a lot larger than condensed\nurban areas. Is there a need to account for the area size difference?</li>\n<li>if let''s say I have census data\ndating back to 4 - 5 census periods,\nhow far can i forecast it into the\nfuture?</li>\n<li>if some of the census zone change\nlightly in boundaries, how can i\naccount for that change?</li>\n<li>What are the methods to validate\ncensus forecasts? for example, if i\nhave data for existing 5 census\nperiods, should I model the first 3\nand test it on the latter two? or is\nthere another way?</li>\n<li>what''s the state of practice in\nforecasting census data, and what are\nsome of the state of the art methods?</li>\n</ul>\n', '1993-09-16 06:04:05', '1970-01-01 02:05:18', '4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', 6),
('12693b29-274c-35ce-d11d-086412a8ea97', 'Bayesian and frequentist reasoning in plain English', '<p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\n', '2009-05-19 17:02:06', '2011-04-15 13:50:05', '3cc58118-77ec-59ed-711a-bb545470e80c', 50),
('12e1b096-3542-127a-1077-d9ebc53aac3d', 'What is the meaning of p values and t values in statistical tests?', '<p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests.  It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the results.  Many computerized tools report test results in terms of "p values" or "t values".</p>\n\n<p>How would you explain the following points to college students taking their first course in statistics:</p>\n\n<ul>\n<li><p>What does a "p-value" mean in relation to the hypothesis being tested?  Are there cases when one should be looking for a high p-value or a low p-value?</p></li>\n<li><p>What is the relationship between a p-value and a t-value?</p></li>\n</ul>\n', '1976-07-23 02:00:42', '1978-11-22 14:08:29', '4bfe55ba-57df-274c-1144-cfd00e0a9b3b', 0),
('13ede5b6-19dd-563e-1980-de64d090a2c2', 'Examples for teaching: Correlation does not mean causation', '<p>There is an old saying: "Correlation does not mean causation". When I teach, I tend to use the following standard examples to illustrate this point:</p>\n\n<ol>\n<li>number of storks and birth rate in Denmark;</li>\n<li>number of priests in America and alcoholism;</li>\n<li>in the start of the 20th century it was noted that there was a strong correlation between ''Number of radios'' and ''Number of people in Insane Asylums''</li>\n<li>and my favorite: <a href="http://en.wikipedia.org/wiki/File%3aPiratesVsTemp%28en%29.svg">pirates cause global warming</a>.</li>\n</ol>\n\n<p>However, I do not have any references for these examples and whilst amusing, they are obviously false.</p>\n\n<p>Does anyone have any other good examples?</p>\n', '1998-11-26 15:40:05', '2002-09-15 11:45:04', '37347d52-7eeb-3f66-ff4d-8601bf3d85d8', 44),
('18baaac1-2b18-2317-b9d2-89b22d10517e', 'Robust nonparametric estimation of hazard/survival functions based on low count data', '<p>We''re trying to use a Gaussian process to model h(t) -- the hazard function -- for a very small initial population, and then fit that using the available data.  While this gives us nice plots for credible sets for h(t) and so on, it unfortunately is also just pushing the inference problem from h(t) to the covariance function of our process.  Perhaps predictably, we have several reasonable and equally defensible guesses for this that all produce different result.  </p>\n\n<p>Has anyone run across any good approaches for addressing such a problem?  Gaussian-process related or otherwise?</p>\n', '1993-09-16 06:04:07', '1979-12-13 00:04:10', '412d134d-6bed-125f-1644-cfd00e0a9b3b', 49),
('197fda97-5a35-781d-0e77-d9ebc53aac3d', 'How Large a Difference Can Be Expected Between Standard GARCH and Asymmetric GARCH Volatility Forecasts?', '<p>I have been using various GARCH-based models to forecast volatility for various North American equities using historical daily data as inputs.</p>\n\n<p>Asymmetric GARCH models are often cited as a modification of the basic GARCH model to account for the ''leverage effect'' i.e. volatility tends to increase more after a negative return than a similarly sized positive return.</p>\n\n<p>What kind of a difference would you expect to see between a standard GARCH and an asymmetric GARCH forecast for a broad-based equity index like the S&amp;P 500 or the NASDAQ-100?</p>\n\n<p>There is nothing particularly special about these two indices, but I think it is helpful to give something concrete to focus the discussion, as I am sure the effect would be different depending on the equities used.</p>\n', '1998-11-26 15:40:03', '1978-03-17 05:18:20', '11643064-22fb-7e14-1399-04b3a3d51ef2', 37),
('1e5e96fa-5d7a-599d-27c2-579daad24557', 'What is your favorite data visualization blog?', '<p>What is the best blog on data visualization?</p>\n\n<p>I''m making this question a community wiki since it is highly subjective.  Please limit each answer to one link.</p>\n\n<hr>\n\n<p><strong><em>Please note the following criteria for proposed answers:</em></strong>  </p>\n\n<blockquote>\n  <p>[A]cceptable answers to questions like this ...need to supply adequate descriptions and reasoned justification. A mere hyperlink doesn''t do it. ...[A]ny future replies [must] meet ...[these] standards; otherwise, <strong>they will be deleted without further comment</strong>. </p>\n</blockquote>\n', '2021-10-08 18:00:48', '1970-01-01 00:08:53', '75de26bb-5a5c-467f-2c8f-4d82f3a13455', 2),
('1f85f165-3ed3-370a-b7d2-89b22d10517e', 'What are some good frameworks for method selection?', '<p>I have been looking into theoretical frameworks for method selection (note: not model selection) and have found very little systematic, mathematically-motivated work. By ''method selection'', I mean a framework for distinguishing the appropriate (or better, optimal) method with respect to a problem, or problem type.</p>\n\n<p>What I have found is substantial, if piecemeal, work on particular methods and their tuning (i.e. prior selection in Bayesian methods), and method selection via bias selection (e.g. <a href="http://portal.acm.org/citation.cfm?id=218546">Inductive Policy: The Pragmatics of Bias Selection</a>). I may be unrealistic at this early stage of machine learning''s development, but I was hoping to find something like what <a href="ftp://ftp.sas.com/pub/neural/measurement.html">measurement theory</a> does in prescribing admissible transformations and tests by scale type, only writ large in the arena of learning problems.</p>\n\n<p>Any suggestions?</p>\n', '1986-06-08 16:53:00', '1970-01-01 00:00:04', '6f10885a-526e-7c3d-2f8f-4d82f3a13455', 2),
('2356a2d8-7751-6f97-66f6-423b4e874d0f', 'What statistical blogs would you recommend?', '<p>What statistical research blogs would you recommend, and why?</p>\n', '1976-07-23 02:00:43', '1977-12-31 02:19:28', '33fad8ab-365a-2815-e078-cf6d4dd5c4a3', 32),
('28cd1599-63bf-785f-4f80-7ed9d1ea06ea', 'Statistical classification of text', '<p>I''m a programmer without statistical background, and I''m currently looking at different classification methods for a large number of different documents that I want to classify into pre-defined categories. I''ve been reading about kNN, SVM and NN. However, I have some trouble getting started. What resources do you recommend? I do know single variable and multi variable calculus quite well, so my math should be strong enough. I also own Bishop''s book on Neural Networks, but it has proven to be a bit dense as an introduction.</p>\n', '2016-07-29 08:24:50', '2007-05-02 11:09:36', '1876fb44-59e4-7596-dcb8-94f47731937d', 7),
('2afb2146-430d-394b-3d35-0028ed526dd0', 'What is the best introductory Bayesian statistics textbook?', '<p>Which is the best introductory textbook for Bayesian statistics?</p>\n\n<p>One book per answer, please.</p>\n', '1986-06-08 16:53:02', '1970-01-01 00:00:44', '33fad8ab-365a-2815-e078-cf6d4dd5c4a3', 21),
('2f43fc9d-1f7a-25fc-5280-7ed9d1ea06ea', 'What is your favorite layman''s explanation for a difficult statistical concept?', '<p>I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explains a difficult statistical concept?</p>\n\n<p>My favorite is <a href="http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Co-integration/Murray93DrunkAndDog.pdf">Murray''s</a> explanation of cointegration using a drunkard and her dog. Murray explains how two random processes (a wandering drunk and her dog, Oliver) can have unit roots but still be related (cointegrated) since their joint first differences are stationary.</p>\n\n<blockquote>\n  <p>The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But\n  periodically she intones "Oliver, where are you?", and Oliver interrupts his aimless\n  wandering to bark. He hears her; she hears him. He thinks, "Oh, I can''t let her get too far\n  off; she''ll lock me out." She thinks, "Oh, I can''t let him get too far off; he''ll wake\n  me up in the middle of the night with his barking." Each assesses how far\n  away the other is and moves to partially close that gap.</p>\n</blockquote>\n', '2004-03-09 07:26:06', '1998-08-16 05:55:36', '15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', 46),
('3304dddb-1b72-607f-25c2-579daad24557', 'What methods can be used to determine the Order of Integration of a time series?', '<p>Econometricians often talk about a time series being <em>integrated with order k, I(k)</em>. <em>k</em> being the minimum number of differences required to obtain a stationary time series.</p>\n\n<p>What methods or statistical tests can be used to determine, given a level of confidence, the <em>order of integration</em> of a time series?</p>\n', '1981-03-29 07:16:58', '1973-01-12 06:19:03', '30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', 37),
('35c3a053-254d-33f3-1b80-de64d090a2c2', 'How do you decide the sample size when polling a large population?ssss', 'Australia is currently having an election and understandably the media reports new political poll results daily. In a country of 22 million what percentage of the population would need to be sampled to get a statistically valid result?\n\n', '1993-09-16 06:04:06', '2021-12-22 17:54:20', '3cc58118-77ec-59ed-711a-bb545470e80c', 359),
('36ac2df8-2b1e-368f-cc1d-086412a8ea97', 'Time series for count data, with counts < 20', '<p>I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we''re currently treating, the number of tests administered, etc.  I''d like to start modeling these counts so that we''re not just guessing whether something is unusual or not.  Unfortunately, I''ve had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:</p>\n\n<p>[image lost to the mists  of time]</p>\n\n<p>[image eaten by a grue]</p>\n\n<p>I''ve found a few articles that address models like this, but I''d greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.</p>\n\n<p><strong>EDIT:</strong>  mbq''s answer has forced me to think more carefully about what I''m asking here; I got too hung-up on the monthly counts and lost the actual focus of the question.  What I''d like to know is: does the (fairly visible) decline from, say, 2008 onward reflect a downward trend in the overall number of cases?  It looks to me like the number of cases monthly from 2001-2007 reflects a stable process; maybe some seasonality, but overall stable.  From 2008 through the present, it looks like that process is changing: the overall number of cases is declining, even though the monthly counts might wobble up and down due to randomness and seasonality.  How can I test if there''s a real change in the process?  And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months?</p>\n\n<p>Whew.  Thanks for bearing with me.</p>\n', '1998-11-26 15:40:04', '1970-01-01 00:04:54', '4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', 44),
('3738ce93-1661-5628-2cc2-579daad24557', 'How should outliers be dealt with in linear regression analysis?', '<p>Often times a statistical analyst is handed a set dataset and asked to fit a model using a technique such as linear regression.  Very frequently the dataset is accompanied with a disclaimer similar to "Oh yeah, we messed up collecting some of these data points -- do what you can".</p>\n\n<p>This situation leads to regression fits that are heavily impacted by the presence of outliers that may be erroneous data. Given the following:</p>\n\n<ul>\n<li><p>It is dangerous from both a scientific and moral standpoint to throw out data for no reason other than it "makes the fit look bad".</p></li>\n<li><p>In real life, the people who collected the data are frequently not available to answer questions such as "when generating this data set, which of the points did you mess up, exactly?"</p></li>\n</ul>\n\n<p>What statistical tests or rules of thumb can be used as a basis for excluding outliers in linear regression analysis?</p>\n\n<p>Are there any special considerations for multilinear regression?</p>\n', '1981-03-29 07:17:02', '2015-03-07 06:04:59', '621899dc-177b-65d2-dbb8-94f47731937d', 4),
('3e51a46c-4153-229c-7f41-c1ec321fe728', 'Group differences on a five point Likert item', '<p>Following on from <a href="http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data" rel="nofollow">this question</a>:\nImagine that you want to test for differences in central tendency between two groups (e.g., males and females)\non a 5-point Likert item (e.g., satisfaction with life: Dissatisfied to Satisfied).\nI think a t-test would be sufficiently accurate for most purposes,\n but that a bootstrap test of differences between group means would often provide more accurate estimate of confidence intervals.\nWhat statistical test would you use?</p>\n', '1986-06-08 16:53:01', '1970-01-01 00:09:09', '694c75bd-3b3d-2034-6f1a-bb545470e80c', 46),
('3e7dd0ec-7ee5-392a-1277-d9ebc53aac3d', 'Intro to statistics for an MD?', '<p>I have a friend who is an MD and wants to refresh his Statistics. So is there any recommended resource online (or offline) ? He did stats ~20 years ago.</p>\n', '2016-07-29 08:24:56', '1970-01-01 02:04:26', '18bce813-54a9-6649-f8c3-4d1f326d5ebd', 40),
('3fed64ec-2f94-798f-0335-60b3ebf808a9', 'Recommended visualization libraries for standalone applications', '<p>Which visualization libraries (plots, graphs, ...) would you suggest to use in a standalone application (Linux, .Net, Windows, whatever). Reasonable performance would be nice as well.</p>\n', '2009-05-19 17:02:07', '1974-11-16 10:10:02', '4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', 32),
('407db4d1-222b-287d-0135-60b3ebf808a9', 'Variance components', '<p>I have a set of $N$ bodies, which is a random sample from a population whose mean and variance I want to estimate. A property of each body is being measured $m_i$ times ($m_i&gt;1$) and different for each body index $i$  identifies which body it is; the property is expected to be distributed around zero). I would like to describe the resulting measurement. Particularly I''m interested in average property value and in the variance.</p>\n\n<p>The average value is simple. First calculate the mean values for each body and then calculate the mean of means.</p>\n\n<p>The variance is more tricky. There are two variances: the variance of measurement and the variance of property values. In order to have an idea on the confidence we have in any single measurement, we need to account for both the sources. Unfortunately, I can''t think of a good method. It is obvious that putting all the numbers in a single pool and calculating the stdev of this pool isn''t a good idea.</p>\n\n<p>Any suggestion?</p>\n\n<p><strong>EDIT</strong>\nColin Gillespie suggests applying Random Effects Model. This model seems to be the right solution for my case, except for the fact that it is described (in Wikipedia) for the cases where each group (body in my case) is sampled equally ($m_i$ is constant for all the bodies), which is not correct in my case</p>\n', '2016-07-29 08:24:53', '2002-06-26 22:48:47', '15f49a4f-6117-2234-1544-cfd00e0a9b3b', 25),
('409dcd1a-426a-23f7-3a35-0028ed526dd0', 'Spatial statistics models: CAR vs SAR', '<p>When would one prefer to use a Conditional Autoregressive model over a Simultaneous Autoregressive model when modelling autocorrelated geo-referenced aerial data?</p>\n', '2004-03-09 07:26:04', '1978-12-06 16:30:53', '60e226cb-63fb-4363-fc4d-8601bf3d85d8', 31),
('40a13723-64bf-409c-a436-6125ec27e2f1', 'Ngo The Tan', 'Hai hai ahaiaiai', '2021-12-16 22:04:33', '2021-12-16 22:04:33', '2ff554db-1bde-573a-701a-bb545470e80c', 83),
('42b9446a-790b-3892-2ac2-579daad24557', 'How to deal with the effect of the order of observations in a non hierarchical cluster analysis?', '<p>When a non-hierarchical cluster analysis is carried out, the order of observations in the data file determine the clustering results, especially if the data set is small (i.e, 5000 observations). To deal with this problem I usually performed a random reorder of data observations. My problem is that if I replicate the analysis n times, the results obtained are different and sometimes these differences are great. </p>\n\n<p>How can I deal with this problem? Maybe I could run the analysis several times and after consider that one observation belong to the group in which more times was assigned. Has someone a better approach to this problem?</p>\n\n<p>Manuel Ramon</p>\n', '1998-11-26 15:40:06', '1979-08-22 04:26:56', '37347d52-7eeb-3f66-ff4d-8601bf3d85d8', 9),
('481183cb-5a6c-795b-68f6-423b4e874d0f', 'Estimating beta-binomial distribution', '<p>Suppose that I culture cancer cells in <em>n</em> different dishes <em>gÃ¢Â‚Â</em>, <em>gÃ¢Â‚Â‚</em>, Ã¢Â€Â¦ , <em>g<sub>n</sub></em> and observe the number of cells <em>n<sub>i</sub></em> in each dish that look different than normal.  The total number of cells in dish <em>g<sub>i</sub></em> is <em>t<sub>i</sub></em>.  There is individual differences between individual cells, but also differences between the populations in different dishes because each dish has a slightly different temperature, amount of liquid, and so on.</p>\n\n<p>I model this as a beta-binomial distribution: <em>n<sub>i</sub></em> ~ Binomial(<em>p<sub>i</sub></em>, <em>t<sub>i</sub></em>) where <em>p<sub>i</sub></em> ~ Beta(<em>ÃŽÂ±</em>, <em>ÃŽÂ²</em>).  Given a number of observations of <em>n<sub>i</sub></em> and <em>t<sub>i</sub></em>, how can I estimate <em>ÃŽÂ±</em> and <em>ÃŽÂ²</em>?</p>\n', '2021-10-08 18:00:54', '1992-09-20 13:31:00', '749718aa-2270-77cc-1444-cfd00e0a9b3b', 36),
('4989ff24-23e7-7b82-b6d2-89b22d10517e', 'In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?', '<p>Am I looking for a better behaved distribution for the independent variable in question, or to reduce the effect of outliers, or something else?</p>\n', '1981-03-29 07:16:59', '1970-02-05 15:08:26', '1beedfca-2f14-1d94-8865-390639088226', 16),
('4db9a514-6d87-2ec1-3b35-0028ed526dd0', 'Feature selection for disease classification based on tests', '<p>I have a dataset of around 100 different subjects</p>\n\n<p>Some of them have a disease, some do not (roughly 60:40 disease:no disease)</p>\n\n<p>They are subjected to a battery of 15 tests, to see if they are outside "normal" ranges.</p>\n\n<p>Just plotting the values for the different tests for disease vs. non-disease as different colours (using <code>matplot()</code> in R), I can see that the different groups follow distinct patterns across the different features.</p>\n\n<p>I then cluster the different groups (using <code>hclust()</code> in R) and if I cut the tree to make two clusters, the two groups separate fairly well into different clusters.</p>\n\n<p>My aim is to devise a set of rules from these tests, so if we test a new patient, we can decide whether or not they have a disease.</p>\n\n<p>So I need a classifier, to decide these rules, i.e. to work out which features to use, and what score cutoffs. What do people recommend?</p>\n', '1981-03-29 07:17:00', '2021-09-28 12:28:04', '4b635464-5ee2-2a80-f103-13a65cc92c97', 11),
('506acf1e-5331-67a9-0d77-d9ebc53aac3d', 'White noise for level, log and log differences data sets', '<p>I am using eviews 7 and I have 3 data sets for DAX stock market index: level (dax), log (ldax), and log differences (dldax). I need to check whether the error terms of these data sets are white noise or not. </p>\n\n<p><strong>Attempt:</strong> I am estimating an equation $y_t=c+\\beta y_{t-1} +v_t$ for each data set. Then, I perform the residuals diagnostics on each. For level data set (dax) looking at the graph of the residuals is enough to see that the error term is not white noise: the variance is not constant. However, I am having difficulties with determining whether the error terms of log (ldax) and log differences (dldax) data sets are white noise or not. The log (ldax) data set is not stationary and the auto correlation is very high while log differences (dldax) data set is stationary and has no auto correlation. I also checked the stationarity of these sets by Augmented Dickey Fuller test. So when I estimate the equation I listed above and check the residuals it looks like the error terms of both data sets are white noise. Here are the graphs for the log: \n<img src="http://i.stack.imgur.com/GAgM3.jpg" alt="log"><br>\nand for the log difference:<br>\n<img src="http://i.stack.imgur.com/z0ghM.jpg" alt="log differences"><br>\nAlso the correlogram of the residuals shows no auto correlation for both. So the error terms of both log and log differences are white noise, but this does not make sense because log data set is not stationary. Am I missing something? </p>\n\n<p>Thank you for your help. If something is confusing or you need more info let me know. I will make necessary edits.</p>\n', '1976-07-23 02:00:41', '1991-01-13 19:23:33', '5bfe5648-65c2-7d1a-f203-13a65cc92c97', 12),
('524751f2-1577-523d-ce1d-086412a8ea97', 'How does gentle boosting differ from AdaBoost?', '<p>There is a variant of boosting called <a href="http://dx.doi.org/10.1214/aos/1016218223">gentleboost</a>.  How does gentle boosting differ from the better-known <a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>?</p>\n', '2021-10-08 18:00:50', '1988-03-13 01:32:21', '1876fb44-59e4-7596-dcb8-94f47731937d', 25),
('5254e49d-4368-3162-29c2-579daad24557', 'What is the single most influential book every statistician should read?', '<p>If you could go back in time and tell yourself to read a specific book at the beginning of your career as a statistician, which book would it be?</p>\n', '2009-05-19 17:02:05', '1983-07-13 06:21:26', '6f10885a-526e-7c3d-2f8f-4d82f3a13455', 7),
('52f3ff99-7fff-436f-9404-c27ed4e2a482', 'ssssa', ' ádasdasdasdasdasd', '2021-12-23 22:10:28', '2021-12-23 22:10:28', '145aebb1-a59e-4f22-93eb-cacd263eaf3f', 2),
('56aaf263-3355-23ea-b8d2-89b22d10517e', 'Computing a bootstrap confidence interval for the prediction error with the percentile and the BCa method', '<p>I have two related questions regarding the computation of a non-parametric bootstrap confidence interval for the prediction error.</p>\n\n<p><strong>Setting:</strong> I have a sample S from a data population P and a learner L, and I want to compute a 95% confidence interval for the 0.632 bootstrap estimator $\\hat{\\theta}_{.632bs}$ of the prediction error $\\theta$ of the classifier C learned by learner L on sample S.</p>\n\n<p><strong>Q1</strong> My first question is whether the following procedure, to compute the confidence interval with the percentile method, is correct. Specifically, the sampling of a test set $S_{test}$ in step 2 to evaluate my classifier on. I have read that, for every observation, you have to keep track of predictions from bootstrap samples not containing that observation. This seems to come down to the same thing according to me?  I compute the confidence interval with the percentile method as follows:</p>\n\n<ol>\n<li>I compute the resubstitution error $\\theta_{resub}$ of the classifier C learned by L\non S</li>\n<li><p>I compute N (e.g. = 1000) bootstrap estimates $\\hat{\\theta}_i^*$ of the prediction error on S as follows:</p>\n\n<p>For i = 1 : 1000 do:</p>\n\n<ul>\n<li>Sample from S with replacement until I have a new sample $S_{train}$ the same size as S</li>\n<li>From all the instances that were not sampled in step 1, I sample again with replacement until I have a sample $S_{test}$ with the same size as S</li>\n<li>I learn a classifier on $S_{train}$ and evaluate it on $S_{test}$. The classifier performance on $S_{test}$ is my bootstrap estimate $\\hat{\\theta}_i^*$.</li>\n</ul></li>\n<li><p>For each sample i of the N bootstrap samples I compute the estimate $\\hat{\\theta}^*_{i,.632bs}$ as\n$\\hat{\\theta}^*_{i,.632bs} = 0.368 \\cdot \\theta_{resub} + 0.632 \\cdot \\hat{\\theta}_i^*$</p></li>\n<li>I sort the 1000 bootstrap estimates</li>\n<li>I select the 25th bootstrap estimate as the lower bound of the confidence interval and the 975th estimate as the upper bound of the confidence interval.</li>\n</ol>\n\n<p><strong>Q2</strong> My second question is how to compute the confidence interval for the 0.632 bootstrap estimator for the prediction error with the bias corrected accelerated (BCa) method. Namely, I find that I have a problem here: I need an estimate of a term $b$ for the bias correction, and for term $a$, the acceleration term (<a href="http://www.tau.ac.il/~saharon/Boot/10.1.1.133.8405.pdf" rel="nofollow">http://www.tau.ac.il/~saharon/Boot/10.1.1.133.8405.pdf</a> p1153-1154). </p>\n\n<ul>\n<li>To compute $b$, I need an estimate of the prediction error $\\hat{\\theta}$ from the complete sample S. I am not sure how to do this. I see several possibilities: The average of $\\hat{\\theta}^*_{i,.632bs}$ over all $i$, the resubstitution error, a hold-out error, perform cross-validation on S...? </li>\n<li>I have the same problem for computing $a$. Here, I need jackknife estimates that use all of the data from S except one instance $x_i$ ($S \\setminus x_i$). What exactly do I compute here? Is this computation the same as computing the leave-one-out cross-validation estimator for the prediction error?</li>\n</ul>\n\n<p>I have tried finding the answers in the literature, but I have difficulty understanding the papers I found, so I hope somebody can help me here. Thank you!</p>\n', '1998-11-26 15:40:07', '1986-05-28 08:22:51', '697be7ba-1738-6adf-298f-4d82f3a13455', 41),
('56dce332-76f0-6b9b-5080-7ed9d1ea06ea', 'Are rejection regions always open/closed?', '<p>Does there exist any agreement on what must rejection regions look like topologically? If we identify the region of "acceptance" with the corresponding confidence interval (or confidence region in dimensions greater than 1) then, according to <a href="http://stats.stackexchange.com/questions/15872/are-confidence-intervals-open-or-closed-intervals">this question</a>, it seems that rejection regions must be open. On the other hand, the rejection region given by the Neyman-Pearson lemma is usually defined in terms of a non-strict inequality, so at least in the continuous case it will be closed. (I know that this is a somewhat bizantine question, at least from a practical point of view.)</p>\n', '1971-05-13 16:24:41', '1970-01-01 00:08:32', '6ed2e962-178c-712c-fd4d-8601bf3d85d8', 39),
('57cdf8c4-47e3-5560-7e41-c1ec321fe728', 'How are confidence intervals for proportions computed under SPSS?', '<p>I wonder whether any of you can help me. SPSS puts confidence intervals in graphs of frequencies and proportions,\nbut it clearly is not using a normal approximation, i.e. it is not using the formula $\\text{CI} = m \\pm 1.96 \\sqrt{(p(1-p)/n}$.\napart from anything else the SPSS interval is not symmetrical around the value.</p>\n\n<p>So what is it doing? I could not track down the formula anywhere, not even searching on the web (which usually\nsolves 99% of problems in life, plus or minus 1%).  :)</p>\n', '1993-09-16 06:04:02', '2019-07-16 02:36:16', '4b635464-5ee2-2a80-f103-13a65cc92c97', 37),
('595e7d51-318c-39e8-1377-d9ebc53aac3d', 'Comparing two binary variables of unequal sizes', '<p>I measured a binary variable from two different populations, and now I''m trying to find out whether the different populations differ with regards to this variable. I could use a Chi-Square test, but that would necessitate that both populations have the same length. Is there an appropriate test for these circumstances? Thank you.</p>\n', '1998-11-26 15:40:08', '1972-11-30 10:49:18', '3d1c5555-3e10-56cf-2b8f-4d82f3a13455', 15),
('5a841b92-791c-78df-5180-7ed9d1ea06ea', 'Working through a clustering problem', '<p>Say I''ve got a program that monitors a news feed and as I''m monitoring it I''d like to discover when a bunch of stories come out with a particular keyword in the title. Ideally I want to know when there are an unusual number of stories clustered around one another.</p>\n\n<p>I''m entirely new to statistical analysis and I''m wondering how you would approach this problem. How do you select what variables to consider? What characteristics of the problem affect your choice of an algorithm? Then, what algorithm do you choose and why?</p>\n\n<p>Thanks, and if the problem needs clarification please let me know.</p>\n', '2004-03-09 07:26:05', '1970-01-01 01:37:34', '5ada22d3-3248-6596-6e1a-bb545470e80c', 50),
('5c7f0ec4-77f2-3794-cf1d-086412a8ea97', 'Using ANN in time series with limited observations', '<p>I''m trying to predict weekly market shares over time with a small data set (190 examples and 4 inputs). </p>\n\n<p>My questions are the following:</p>\n\n<ol>\n<li><p>Is there a particular technique for small number of observations? (i.e., I heard that you could train a network with white noise to avoid the cross-validation.) </p></li>\n<li><p>Do I have to filter data when i''m working with time series? (i.e., account for seasonal dummies) </p></li>\n</ol>\n', '2021-10-08 18:00:51', '1970-07-13 21:47:28', '6f2979d3-58ec-3673-f303-13a65cc92c97', 1),
('5de704c2-573f-1a71-1a80-de64d090a2c2', 'Comparison of two Simpson indices using t-test', '<p>I would like to compare two Simpson Indices from two different populations. I have calculated their variance, as it is done in the original paper by Simpson regarding measures of diversity and I have calculated a confidence interval for each of them using the formula:</p>\n\n<p>$(S-2\\sqrt{\\text{var}},S+2\\sqrt{\\text{var}})$, </p>\n\n<p>as suggested in a published paper.</p>\n\n<p>What I would like to find is a p-value of the null hypothesis that the two indices are equal.</p>\n\n<p>I have read that someone can do a Welch t-test to compare them, but I haven''t found any single paper or book with such an application. </p>\n\n<p>My questions regarding this application are:</p>\n\n<p>1) In Welch t-test the variances in the denominator are divided by $n_1$ and $n_2$ respectively, since it is the SE of the mean. I guess in this case and based on the formula for the CI we shouldn''t divide by $n$ and have just the square root of the sum of variances. Correct?   </p>\n\n<p>2) The degrees of freedom for a simple t-test are $n_1+n_2-2$, while for the Welch t-test is quite a complicated formula which gives a result close but not the same as $n_1+n_2-2$. Which one should be used?</p>\n\n<p>3) By $n_1$ and $n_2$ above we mean the number of different categories in each population rather than the total number in each case. Correct?</p>\n\n<p>I would be very grateful if someone can help me with these questions and even more if someone can provide some sort of documentation so I can justify my analysis. </p>\n\n<p>Reference:</p>\n\n<p>Simpson, E. H. (1949), Measurement of diversity. <em>Nature</em>, <strong>163</strong>, 688 (<a href="http://people.wku.edu/charles.smith/biogeog/SIMP1949.pdf" rel="nofollow">pdf</a>)</p>\n', '2016-07-29 08:24:54', '2012-08-09 15:04:46', '76753dc8-4972-2ecd-2e8f-4d82f3a13455', 28),
('5f03d745-7702-1c85-2bc2-579daad24557', 'How to tell if something happened in a data set which monitors a value over time', '<p>I have a data set where a series of measurements are being taken each week. In general the data set shows a +/- 1mm change each week with a mean measurement staying at about 0mm. In plotting the data this week it appears that some noticeable movement has occurred at two points and looking back at the data set, it is also possible that movement occurred last week as well. \nWhat is the best way of looking at this data set to see how likely it is that the movements that have been seen are real movements rather than just some effect caused by the natural tolerance in the readings.</p>\n\n<p><strong>Edit</strong></p>\n\n<p>Some more information on the data set. Measurements have been taken at 39 locations which should behave in a similar way although only some of the points may show signs of movement. At each point the readings have now been taken 10 times on a bi-weekly basis and up until the most recent set of readings the measurements were between -1mm and 1mm. The measurements can only be taken with mm accuracy so we only receive results to the nearest mm. The results for one of the points showing a movement is 0mm, 1mm, 0mm, -1mm, -1mm, 0mm, -1mm, -1mm, 1mm, 3mm. We are not looking for statistically significant information, just an indicator of what might have occurred. The reason is that if a measurement reaches 5mm in a subsequent week we have a problem and we''d like to be forewarned that this might occur. </p>\n', '1976-07-23 02:00:44', '2020-05-14 06:38:16', '697be7ba-1738-6adf-298f-4d82f3a13455', 51),
('5fab4ff7-1b8e-64d0-1880-de64d090a2c2', 'Building a probability distribution function from observation', '<p>There are N players and M objects, each of the objects has a value. Each player has a strategy in choosing an object. Each round a player will choose an object, many players can choose the same object. However the value of each object is divided evenly among every player that has chosen it. There will be 9000 rounds(choices) per game. Our goal is to maximize the values that we accumulate at the end of the game.</p>\n\n<p>Question: how can I build a probability distribution function for each playing assuming that their decisions are random variables?</p>\n\n<p>Current Approach: My current approach is to count the frequency of a player choosing a specific object and dividing by the total number of rounds, that would give a probability a player is likely to choose that specific object. </p>\n\n<p>Problem: With each player playing aggressively trying to be unpredictable as possible(noise), with my current approach the probability distribution functions are not accurate(9000 rounds doesn''t seem to be enough data). Is there a better way to build these distribution functions?</p>\n\n<p>Note: I''ve read somewhere that (Bayes model and HMM) are more superior than frequency counts, but I am not sure how to adapt it to this situation.</p>\n', '1993-09-16 06:04:03', '2006-12-07 09:54:35', '5e6695e2-5aad-3241-6c1a-bb545470e80c', 10),
('62563348-7bd5-6519-3c35-0028ed526dd0', '2 x 2 between-subjects design ANCOVA with non-sig. interaction effect; but profile plot shows curves crossing', '<p>I''m running a 2X2 between-subjects design ANCOVA and wonder about one peculiar thing: The ANCOVA output table reveals no significant interaction effect, albeit the profile plot shows two lines that cross each other, i.e. an disordinal interaction. So how to reconcile these apparently contradictory findings? Might I presume the existence of an interaction effect (which would make a lot of sense by the way..) based on the plot despite a lack of statistical significance? And why is it non-significant; perhaps because of a lack of statistical power?</p>\n', '1971-05-13 16:24:43', '2001-06-19 06:50:08', '5e6695e2-5aad-3241-6c1a-bb545470e80c', 21),
('62af2231-78a7-7caa-cd1d-086412a8ea97', 'I need a new and improved DFT', '<p>The traditional Discrete Fourier Transform (DFT) and its cousin, the FFT, produce bins that are spaced equally. In other words, you get something like the first 10 hertz in the first bin, 10.1 through 20 in the second, etc. However, I need something a little different. I want the range of frequency covered by each bin to increase geometrically. Suppose I select a multiplier of 1.5. Then we have 0 through 10 in the first bin, I want 11 through 25 in the second bin, 26 through 48 in the third, etc. Is it possible to modify the DFT algorithm to behave in this fashion?</p>\n', '2016-07-29 08:24:52', '1983-02-08 01:47:03', '4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', 5),
('6485b64f-7835-65a4-8041-c1ec321fe728', 'Comparing patient and control group results of a survey', '<p>I have data from a survey given to a patient group and control group. Both groups completed the same exact survey (Likert scale). One group has an n of only 5 or 6, the other n was about 15.</p>\n\n<p>What test should I conduct to determine if there is a significant difference between the two groups answers on the survey? Would this be an independent samples t-test? Do I need to do anything to address the difference in sample size?</p>\n\n<p>To make matters worse, there is some data missing from 3 participants who have 1 or two answers missing on the survey. </p>\n\n<p>Is this data even worth analyzing or is there not enough power?</p>\n', '1981-03-29 07:17:01', '2021-10-18 16:21:50', '7ac8b8cf-6a0d-6fa3-d9b8-94f47731937d', 34),
('677132b4-2a33-68d1-26c2-579daad24557', 'How to approximate measurement uncertainty?', '<p>At the moment I use standard deviation of the mean to estimate uncertainty:</p>\n\n<p><img src="http://upload.wikimedia.org/math/8/a/e/8ae657bf495238caf69d82d754e70ed8.png" alt="definition = stdev over sq.root of N"></p>\n\n<p>where <em>N</em> is in hundreds and mean is a time series (monthly) mean. I\npresent it then like this: <img src="http://rogercortesi.com/eqn/tempimagedir/eqn1898.png" alt="mean plus-minus stdev about the mean"> for each element (month) in the (annual) time series.</p>\n\n<p>Is this valid? Is this appropriate for time series?</p>\n', '2009-05-19 17:02:04', '1977-02-24 03:48:24', '2ff554db-1bde-573a-701a-bb545470e80c', 28),
('6a763a51-4322-5c5a-b5d2-89b22d10517e', 'Is this a correct procedure for feature selection using cross-validation?', '<p>I was looking for information about feature selection and crossvalidation, when I found this post:</p>\n\n<p><a href="http://stats.stackexchange.com/questions/2306/feature-selection-for-final-model-when-performing-cross-validation-in-machine">Feature selection for &quot;final&quot; model when performing cross-validation in machine learning</a>.</p>\n\n<p>where there is a discussion about how to use feature selection on cross-validation. I saw that the procedure described is somewhat different from what I use, and I would like to ask if what I do is correct.</p>\n\n<p>What I usually do for feature selection is to perform a search for the best features using the area under roc curve in cross-validation as a function to optimize. I usually perform this search few times, every time with a different cross-validation partitions but with the same number of folds ( 3 to 5) as recomended by Ron kohavi talk ( IJCAI 95). The feature set that appeared as the best one more times is the one I choose. Then, with this feature set I perform 10 fold cross-validation for accuracy prediction. I would appreciate any commentson this procedure. Thanks,</p>\n\n<p>Jorge</p>\n', '1986-06-08 16:52:59', '1972-01-20 17:01:23', '37347d52-7eeb-3f66-ff4d-8601bf3d85d8', 39),
('6b550b8f-7ff9-51bc-0f77-d9ebc53aac3d', 'Rescaling AIC values to select the model', '<p>I have a set of 492 observations that I fit between a exponential or a logarithimic curve.\nBoth curves share the same amount of explanatory variables.</p>\n\n<p>My question lies on, to use the rule of thumb that DeltaAIC &lt; 2, we have to rescale the data to vary between 1 to 10?</p>\n\n<p>Here is the result table, AIC for exponential and Log curves, and their sum(residues^2). </p>\n\n<pre><code>AIC expon  AIC log    Res^2 expo  Res^2 log \n-798.6752  -346.68    0.9005307   4.5773081\n-711.1705  -950.659   1.2336707   0.5212761\n-562.4169  -685.1748  2.1066076   1.3545968\n-800.1217  -390.1025  0.8958573   3.9153901\n-688.1213  -886.8839  1.3403152   0.655689\n</code></pre>\n\n<p>For example line 3:  -685-(-562) = 123 that''s the Delta AIC, should I rescale <em>Dividing it by 100</em>? resulting in Delta AIC - 1,23 thus both models are equally explicative?</p>\n', '1993-09-16 06:04:04', '1982-12-09 15:55:38', '609d0ea1-772a-1b4a-8965-390639088226', 5),
('6c5b9fe7-14c9-3bb5-b4d2-89b22d10517e', 'IID assumption for $Y_t=X_t-X_{t-1}$', '<p>I have some time series data, $X_t$, and I need to fit a heavy tail distribution to the first difference i.e. $Y_t=X_t-X_{t-1}$. Prior to fit this distribution, I need to test the iid (identical independent distribution) assumption. So far, I check the ACF (autocorrelation function) plot of $Y_t$ to see if there is any serial autocorrelation. This shows no significant autocorrelation. I also applied LjungÃ¢Â€Â“Box test and McLeod Li test to $Y_t$ in order to check independency and conditional heteroscedascity (ARCH), respectively. Neither of these two tests was significant. Hence, there is no evidence of dependency and presence of conditional heteroscedascity in $Y_t$. Moreover, from the plot of $Y_t$, I can conclude that the unconditional variance of $Y_t$ is constant over time.  Finally, I performed runs test over $Y_t$ to check for randomness. Based on this test, I couldnÃ¢Â€Â™t reject the randomness of $Y_t$.   </p>\n\n<p>Question: Under these conditions, can I claim that the $Y_t$''s are iid? If not, what other test should I perform in order to show the iid assumption?</p>\n', '2021-10-08 18:00:49', '1970-01-01 00:00:06', '7ac8b8cf-6a0d-6fa3-d9b8-94f47731937d', 29),
('6c80a684-1cb5-41af-ff34-60b3ebf808a9', 'Distribution of random effects', '<p>Why do we usually assume that random effects come from a normal distribution? Can we assume another distribution? Or maybe because the CLT indicates that a random effect is normally distributed?</p>\n', '2021-10-08 18:00:47', '1987-07-09 08:42:15', '15f49a4f-6117-2234-1544-cfd00e0a9b3b', 57),
('6f51ea57-13b2-139f-1177-d9ebc53aac3d', 'Motivation for Kolmogorov distance between distributions', '<p>There are many ways to measure how similar two probability distributions are.  Among methods which are popular (in different circles) are:</p>\n\n<ol>\n<li><p>the Kolmogorov distance: the sup-distance between the distribution functions;</p></li>\n<li><p>the Kantorovich-Rubinstein distance: the maximum difference between the expectations w.r.t. the two distributions of functions with Lipschitz constant $1$, which also turns out to be the $L^1$ distance between the distribution functions;</p></li>\n<li><p>the bounded-Lipschitz distance: like the K-R distance but the functions are also required to have absolute value at most $1$.</p></li>\n</ol>\n\n<p>These have different advantages and disadvantages.  Only convergence in the sense of 3. actually corresponds precisely to convergence in distribution; convergence in the sense of 1. or 2. is slightly stronger in general.  (In particular, if $X_n=\\frac{1}{n}$ with probability $1$, then $X_n$ converges to $0$ in distribution, but not in the Kolmogorov distance. However, if the limit distribution is continuous then this pathology doesn''t occur.) </p>\n\n<p>From the perspective of elementary probability or measure theory, 1. is very natural because it compares the probabilities of being in some set. A more sophisticated probabilistic perspective, on the other hand, tends to focus more on expectations than probabilities. Also, from the perspective of functional analysis, distances like 2. or 3. based on duality with some function space are very appealing, because there is a large set of mathematical tools for working with such things.</p>\n\n<p>However, my impression (correct me if I''m wrong!) is that in statistics, the Kolmogorov distance is the usually preferred way of measuring similarity of distributions.  I can guess one reason: if one of the distributions is discrete with finite support -- in particular, if it is the distribution of some real-world data -- then the Kolmogorov distance to a model distribution is easy to compute.  (The K-R distance would be slightly harder to compute, and the B-L distance would probably be impossible in practical terms.)</p>\n\n<p>So my question (finally) is, are there other reasons, either practical or theoretical, to favor the Kolmogorov distance (or some other distance) for statistical purposes?</p>\n', '2016-07-29 08:24:55', '1991-09-23 11:57:09', '749718aa-2270-77cc-1444-cfd00e0a9b3b', 37),
('73dc79e1-1ccb-35a6-0235-60b3ebf808a9', 'How can I tell whether my data is skewed due to sampling error?', '<p>I have some psychological data (N=100, 2 conditions) in which across 55 questions participants made an estimate, and then after a manipulation they made an additional estimate. I then calculated how much their estimate gained or lost on the second round. There were two manipulations that occurred before the second round, and each participant received one of them.</p>\n\n<p>I found that in both conditions estimates (on average) improved in the second round compared to the first. However, the median ''improvement'' from round 1 to round 2 was negative. This was true across both conditions. </p>\n\n<p>It''s been suggested to me that the reason for this was just sampling error. Is there a way for me to investigate the plausibility of this suggestion without re-doing my study?</p>\n\n<p>In response to Erik''s question, here is a link to the raw data: <a href="https://dl.dropbox.com/u/53771404/estimation_data.xlsx" rel="nofollow">https://dl.dropbox.com/u/53771404/estimation_data.xlsx</a>.</p>\n\n<p>Column 1 = Mean error of original estimates.<br>\nColumn 2 = Mean error of second estimates.<br>\nColumn 3 = Median error of original estimates.<br>\nColumn 4 = Median error of second estimates.  </p>\n\n<p>This is just manipulation condition 1, but the second condition had very similar results.</p>\n', '1971-05-13 16:24:42', '1970-01-01 02:03:38', '6ed2e962-178c-712c-fd4d-8601bf3d85d8', 22),
('7ad34860-1ac8-7ad7-d01d-086412a8ea97', 'Designing a uniformly most powerful test.', '<blockquote>\n  <p>Let $X_1, ... , X_n$ be iid normally distributed random variables $N(\\mu, \\sigma^2)$, $\\mu \\in \\Bbb{R}$, $ \\sigma^2 &gt; 0$. </p>\n  \n  <p>a) Design a uniformly most powerful test with significance level $\\alpha$ for testing $H_0: \\sigma^2 = \\sigma^2_0$ vs $H_1: \\sigma^2 &gt; \\sigma^2_0$.</p>\n  \n  <p>b) Give a formula for the power of the test. </p>\n</blockquote>\n\n<p>Let $\\sigma^2_1 &gt; \\sigma^2_0$. </p>\n\n<p>$\\Lambda = \\frac{L(\\sigma^2_0)}{L(\\sigma^2_1)}$ = $\\frac{(\\frac{1}{\\sqrt{2\\pi}})^n (1/\\sigma^2_0)^{n/2} e^{-\\sum (X_i - \\mu)^2/\\sigma^2_0}}{(\\frac{1}{\\sqrt{2\\pi}})^n (1/\\sigma^2_1)^{n/2} e^{-\\sum (X_i - \\mu)^2/\\sigma^2_1}} \\leq k$ for some $k &lt; 1$. </p>\n\n<p>$\\implies e^{\\frac{-\\sum (X_i - \\mu)^2}{2\\sigma^2_0} + \\frac{\\sum (X_i - \\mu)^2}{2\\sigma^2_1}} \\leq (\\frac{\\sigma^2_0}{\\sigma^2_1})^{n/2}k$</p>\n\n<p>Let $k_1 = (\\frac{\\sigma^2_0}{\\sigma^2_1})^{n/2}k$. </p>\n\n<p>We have $ \\frac{-\\sum (X_i - \\mu)^2}{2\\sigma^2_0} + \\frac{\\sum (X_i - \\mu)^2}{2\\sigma^2_1} \\leq \\ln(k_1)$ </p>\n\n<p>$\\implies \\sum (X_i - \\mu)^2[\\frac{1}{2\\sigma^2_1} - \\frac{1}{2\\sigma^2_0}] \\leq \\ln(k_1)$</p>\n\n<p>$\\implies \\sum (X_i - \\mu)^2 \\geq \\ln(k_1)[\\frac{1}{2\\sigma^2_1} - \\frac{1}{2\\sigma^2_0}]^{-1}$</p>\n\n<p>$\\implies \\frac{\\sum (X_i - \\mu)^2}{\\sigma^2_0} \\geq \\ln(k_1)[\\frac{1}{2\\sigma^2_1} - \\frac{1}{2\\sigma^2_0}]^{-1}(\\frac{1}{\\sigma^2_0})$.</p>\n\n<p>Let $k_2 = \\ln(k_1)[\\frac{1}{2\\sigma^2_1} - \\frac{1}{2\\sigma^2_0}]^{-1}(\\frac{1}{\\sigma^2_0})$.</p>\n\n<p>We know that $\\frac{\\sum (X_i - \\mu)^2}{\\sigma^2_0}$ has chi-square with n degrees of freedom.</p>\n\n<p>So we choose $k_2$ such that $P_{H_0}( \\frac{\\sum (X_i - \\mu)^2}{\\sigma^2_0} \\geq k_2) = \\alpha$</p>\n\n<p>b) $P_{H_1}( \\frac{\\sum (X_i - \\mu)^2}{\\sigma^2_0} \\geq k_2)$ $= 1 - \\int^{k_2}_0$ $(\\frac{1}{\\Gamma(n/2)2^{n/2}})x^{n/2-1}e^{-x/2} dx$. </p>\n\n<p>Are my answers correct? </p>\n\n<p>Thanks in advance</p>\n', '2021-10-08 18:00:52', '1998-11-30 08:32:21', '75de26bb-5a5c-467f-2c8f-4d82f3a13455', 30),
('7b666cc3-1b67-6037-67f6-423b4e874d0f', 'Bayesian inference over an unknown variance', '<p>I am observing a random variable $X \\in \\mathbb{R}$ which can be assumed to be normally distributed with mean $\\mu$ and variance $\\sigma^2$. I am interested in fitting a posterior distribution over the <strong>unknown variance</strong> which, according to wikipedia, can be given in closed form by an inverse-gamma. My question is, given samples $\\{x_1,...,x_n\\}$ how can I fit this distribution, that is, calculate the parameters $\\alpha$ and $\\beta$? </p>\n\n<p><strong>edit:</strong></p>\n\n<p>I found <a href="http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf" rel="nofollow">this pdf</a>, which gives the equations for the parameter updates, and much more relevant info. </p>\n', '2021-10-08 18:00:53', '2021-05-11 18:13:03', '6207a9a7-3bf9-7288-8565-390639088226', 22),
('7bd9cf39-3f55-3a7b-0035-60b3ebf808a9', 'Rerum voluptate inventore asperiores.', 'Rerum voluptate inventore asperiores.', '2016-07-29 08:24:51', '1982-06-20 05:46:23', '1fee0bc4-513a-2c25-004e-8601bf3d85d8', 18),
('ac809355-11d4-4ea4-8ff6-1de0385f9bdb', 'sssssaasdasdasdsssss', 'OIUSIOUSIOUSOIUSOISUOSIUSOIUSIOU', '2021-12-23 22:09:31', '2021-12-23 22:09:48', '145aebb1-a59e-4f22-93eb-cacd263eaf3f', 4),
('b2f86f84-7d9d-48aa-954e-879a723d3732', 'ngo The Tan', 'Masiuasiasiu asiuyauyasas', '2021-12-15 00:27:03', '2021-12-15 00:27:03', '11643064-22fb-7e14-1399-04b3a3d51ef2', 167),
('cadda7e7-cd37-4add-8699-6808e258d60e', 'Ngo The Tans', 'asiodyas AUayauuusu opaoapopa', '2021-12-15 00:27:11', '2021-12-15 00:27:11', '18bce813-54a9-6649-f8c3-4d1f326d5ebd', 4);

-- 
-- Dumping data for table answer
--
INSERT INTO answer VALUES
('0ef51c1d-f335-41a9-8b96-b0034f26263e', 'sádasdasdasd', '2021-12-22 10:48:39', '2021-12-22 10:48:39', '37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '40a13723-64bf-409c-a436-6125ec27e2f1'),
('11452b0c-768e-5ff7-0d63-eeb1d8ed8cef', '<p>The R-project</p>\n\n<p><a href="http://www.r-project.org/">http://www.r-project.org/</a></p>\n\n<p>R is valuable and significant because it was the first widely-accepted Open-Source alternative to big-box packages.  It''s mature, well supported, and a standard within many scientific communities.</p>\n\n<ul>\n<li><a href="http://www.inside-r.org/why-use-r">Some reasons why it is useful and valuable</a> </li>\n<li>There are some nice tutorials <a href="http://gettinggeneticsdone.blogspot.com/search/label/ggplot2">here</a>.</li>\n</ul>\n', '1983-08-28 03:38:22', '1990-01-17 09:27:38', '1876fb44-59e4-7596-dcb8-94f47731937d', '4db9a514-6d87-2ec1-3b35-0028ed526dd0'),
('1152da18-24d8-65b6-0380-e8e2a817c80e', '<p>See my response to <a href="http://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on/2252450#2252450">"Datasets for Running Statistical Analysis on"</a> in reference to datasets in R.</p>\n', '1970-01-01 02:45:15', '1994-05-22 06:09:51', '4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '7b666cc3-1b67-6037-67f6-423b4e874d0f'),
('11f7089b-33e6-371a-f817-7000f455f3d5', '<p>Machine Learning seems to have its basis in the pragmatic - a Practical observation or simulation of reality.  Even within statistics, mindless "checking of models and assumptions" can lead to discarding methods that are useful.</p>\n\n<p>For example, years ago, the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome.  Technically, that''s a bad approach, but practically, it worked.</p>\n', '2016-11-02 13:55:52', '1989-05-15 00:34:02', '4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '5c7f0ec4-77f2-3794-cf1d-086412a8ea97'),
('142cb08f-7c31-21fa-8e90-67245e8b283e', '<p>Two projects spring to mind:</p>\n\n<ol>\n<li><a href="http://www.mrc-bsu.cam.ac.uk/bugs/">Bugs</a> - taking (some of) the pain out of Bayesian statistics. It allows the user to focus more on the model and a bit less on MCMC.</li>\n<li><a href="http://www.bioconductor.org/">Bioconductor</a> - perhaps the most popular statistical tool in Bioinformatics. I know it''s a R repository, but there are a large number of people who want to learn R, just for Bioconductor. The number of packages available for cutting edge analysis, make it second to none.</li>\n</ol>\n', '1970-01-01 00:07:13', '1970-01-01 00:00:04', '4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '4db9a514-6d87-2ec1-3b35-0028ed526dd0'),
('14c0ce35-1687-459c-1acb-66948daf6128', '<p><a href="http://www.gapminder.org/data/">Gapminder</a> has a number (430 at the last look) of datasets, which may or may not be of use to you.</p>\n', '1977-12-17 05:25:03', '1970-01-01 00:16:31', '11fbda18-5d9f-396c-1299-04b3a3d51ef2', '5fab4ff7-1b8e-64d0-1880-de64d090a2c2'),
('17120d02-6ab5-3e43-18cb-66948daf6128', '<p>For doing a variety of MCMC tasks in Python, there''s <a href="http://code.google.com/p/pymc/">PyMC</a>, which I''ve gotten quite a bit of use out of.  I haven''t run across anything that I can do in BUGS that I can''t do in PyMC, and the way you specify models and bring in data seems to be a lot more intuitive to me.</p>\n', '2006-12-28 07:39:39', '1974-04-15 19:35:14', '4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '407db4d1-222b-287d-0135-60b3ebf808a9'),
('1ded3dd7-32db-5bc9-9190-67245e8b283e', '<p><a href="http://mathforum.org/workshops/sum96/data.collections/datalibrary/data.set6.html" rel="nofollow">http://mathforum.org/workshops/sum96/data.collections/datalibrary/data.set6.html</a></p>\n', '1970-01-01 00:01:23', '1970-01-01 00:57:34', '5ef3b4c9-75c9-482e-1699-04b3a3d51ef2', '7bd9cf39-3f55-3a7b-0035-60b3ebf808a9'),
('1f22399e-14b4-5c2b-8132-6a1673ffca7c', '<p>I recommend R (see <a href="http://cran.r-project.org/web/views/TimeSeries.html">the time series view on CRAN</a>).  </p>\n\n<p>Some useful references:</p>\n\n<ul>\n<li><a href="http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf">Econometrics in R</a>, by Grant Farnsworth</li>\n<li><a href="http://stackoverflow.com/questions/1714280/multivariate-time-series-modelling-in-r/1715488#1715488">Multivariate time series modelling in R</a></li>\n</ul>\n', '1981-06-06 20:37:24', '1970-01-01 00:00:04', '15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', '42b9446a-790b-3892-2ac2-579daad24557'),
('1f74cd3f-6de4-5324-0180-e8e2a817c80e', '<p>If your mean value for the Poisson is 1500, then you''re very close to a normal distribution; you might try using that as an approximation and then modelling the mean and variance separately.</p>\n', '1980-07-28 02:57:53', '1983-09-10 12:11:28', '68069d95-76b4-1dce-f503-13a65cc92c97', '18baaac1-2b18-2317-b9d2-89b22d10517e'),
('1f8d7e46-1d1e-335f-79dd-867949feb8b5', '<p>A quote from <a href="http://en.wikipedia.org/wiki/Standard_deviation" rel="nofollow">Wikipedia</a>.</p>\n\n<blockquote>\n  <p>It shows how much variation there is from the "average" (mean, or expected/budgeted value). A low standard deviation indicates that the data points tend to be very close to the mean, whereas high standard deviation indicates that the data is spread out over a large range of values.</p>\n</blockquote>\n', '1982-11-01 21:24:41', '1992-06-08 10:56:33', '2ff554db-1bde-573a-701a-bb545470e80c', '12e1b096-3542-127a-1077-d9ebc53aac3d'),
('205a3e0a-13fe-3557-8232-6a1673ffca7c', '<p>R is great, but I wouldn''t really call it "windows based" :) That''s like saying the cmd prompt is windows based. I guess it is technically in a window...</p>\n\n<p>RapidMiner is far easier to use [1]. It''s a free, open-source, multi-platform, GUI. Here''s a video on time series forecasting:</p>\n\n<p><a href="http://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1">http://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1</a></p>\n\n<p>Also, don''t forget to read:</p>\n\n<p><a href="http://www.forecastingprinciples.com/">http://www.forecastingprinciples.com/</a></p>\n\n<p>[1] No, I don''t work for them. </p>\n', '1972-01-11 03:39:00', '1970-01-01 00:00:38', '4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '7ad34860-1ac8-7ad7-d01d-086412a8ea97'),
('24629593-3d34-32fd-997d-e8c757976496', '<p>The <a href="http://en.wikipedia.org/wiki/Mersenne_twister" rel="nofollow">Mersenne Twister</a> is one I''ve come across and used before now.</p>\n', '1970-01-01 00:01:38', '1984-03-04 02:43:40', '6f10885a-526e-7c3d-2f8f-4d82f3a13455', '56aaf263-3355-23ea-b8d2-89b22d10517e'),
('247986de-4165-651a-9490-67245e8b283e', '<p>A standard deviation is the square root of the second central moment of a distribution. A central moment is the expected difference from the expected value of the distribution. A first central moment would usually be 0, so we define a second central moment as the expected value of the squared distance of a random variable from its expected value. </p>\n\n<p>To put it on a scale that is more in line with the original observations, we take the square root of that second central moment and call it the standard deviation. </p>\n\n<p>Standard deviation is a property of a population. It measures how much average "dispersion" there is to that population. Are all the obsrvations clustered around the mean, or are they widely spread out? </p>\n\n<p>To estimate the standard deviation of a population, we often calculate the standard deviation of a "sample" from that population. To do this, you take observations from that population, calculate a mean of those observations, and then calculate the square root of the average squared deviation from that "sample mean". </p>\n\n<p>To get an unbiased estimator of the variance, you don''t actually calculate the average squared deviation from the sample mean, but instead, you divide by (N-1) where N is the number of observations in your sample. Note that this "sample standard deviation" is not an unbiased estimator of the standard deviation, but the square of the "sample standard deviation" is an unbiased estimator of the variance of the population. </p>\n', '1973-09-28 05:54:20', '1999-03-04 13:26:53', '3cc58118-77ec-59ed-711a-bb545470e80c', '197fda97-5a35-781d-0e77-d9ebc53aac3d'),
('284ccb05-337b-7e6f-9090-67245e8b283e', '<p>You don''t need to install any packages because this is possible with base-R functions.  Have a look at <a href="http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/ts/html/arima.html">the arima function</a>.  </p>\n\n<p>This is a basic function of <a href="http://en.wikipedia.org/wiki/Box%E2%80%93Jenkins">Box-Jenkins analysis</a>, so you should consider reading one of the R time series text-books for an overview; my favorite is Shumway and Stoffer. "<a href="http://rads.stackoverflow.com/amzn/click/0387293175">Time Series Analysis and Its Applications: With R Examples</a>".</p>\n', '1970-01-01 00:15:42', '2019-01-11 13:07:23', '1beedfca-2f14-1d94-8865-390639088226', '6485b64f-7835-65a4-8041-c1ec321fe728'),
('2b850d3d-6722-3f01-1bcb-66948daf6128', '<p>From <a href="http://en.wikipedia.org/wiki/Random_variable" rel="nofollow">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>In mathematics (especially probability\n  theory and statistics), a random\n  variable (or stochastic variable) is\n  (in general) a measurable function \n  that maps a probability space into a\n  measurable space. Random variables\n  mapping all possible outcomes of an\n  event into the real numbers are\n  frequently studied in elementary\n  statistics and used in the sciences to\n  make predictions based on data\n  obtained from scientific experiments.\n  In addition to scientific\n  applications, random variables were\n  developed for the analysis of games of\n  chance and stochastic  events. The\n  utility of random variables comes from\n  their ability to capture only the\n  mathematical properties necessary to\n  answer probabilistic  questions.</p>\n</blockquote>\n\n<p>From <a href="http://cnx.org/content/m13418/latest/" rel="nofollow">cnx.org</a>:</p>\n\n<blockquote>\n  <p>A random variable is a function, which assigns unique numerical values to all possible\n  outcomes of a random experiment under fixed conditions. A random variable is not a \n  variable but rather a function that maps events to numbers.</p>\n</blockquote>\n', '1977-06-05 12:36:31', '1999-04-08 18:12:37', '621899dc-177b-65d2-dbb8-94f47731937d', '7bd9cf39-3f55-3a7b-0035-60b3ebf808a9'),
('2bbbd485-6df2-536a-7e32-6a1673ffca7c', '<p>The assumption of normality is just the supposition that the underlying <a href="http://en.wikipedia.org/wiki/Random_variable">random variable</a> of interest is distributed <a href="http://en.wikipedia.org/wiki/Normal_distribution">normally</a>, or approximately so.  Intuitively, normality may be understood as the result of the sum of a large number of independent random events.</p>\n\n<p>More specifically, normal distributions are defined by the following function:</p>\n\n<p><img src="http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png" alt="alt text"></p>\n\n<p>where $\\mu$ and $\\sigma^2$ are the mean and the variance, respectively, and which appears as follows:</p>\n\n<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png" alt="alt text"></p>\n\n<p>This can be checked in <a href="http://en.wikipedia.org/wiki/Normality_test">multiple ways</a>, that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected <a href="http://en.wikipedia.org/wiki/Q-Q_plot">quantile distribution</a>).</p>\n', '1976-12-23 13:41:40', '1984-07-23 18:58:49', '3d6207a1-1e88-4e30-f403-13a65cc92c97', '12e1b096-3542-127a-1077-d9ebc53aac3d'),
('2bf413d3-7a2c-217a-78dd-867949feb8b5', '<p>Standard deviation is a number that represents the "spread" or "dispersion" of a set of data. There are other measures for spread, such as range and variance. </p>\n\n<p>Here are some example sets of data, and their standard deviations:</p>\n\n<pre><code>[1,1,1]     standard deviation = 0   (there''s no spread)  \n[-1,1,3]    standard deviation = 1.6 (some spread) \n[-99,1,101] standard deviation = 82  (big spead)\n</code></pre>\n\n<p>The above data sets have the same mean. </p>\n\n<p>Deviation means "distance from the mean".</p>\n\n<p>"Standard" here means "standardized", meaning the standard deviation and mean are in the same units, unlike variance. </p>\n\n<p>For example, if the mean height is 2 <strong>meters</strong>, the standard deviation might be 0.3 <strong>meters</strong>, whereas the variance would be 0.09 <strong>meters squared</strong>. </p>\n\n<p>It is convenient to know that <a href="http://en.wikipedia.org/wiki/Chebyshev%27s_inequality">at least 75%</a> of the data points <em>always</em> lie within 2 standard deviations of the mean (or <a href="http://en.wikipedia.org/wiki/68-95-99.7_rule">around 95%</a> if the distribution is Normal).</p>\n\n<p>For example, if the mean is 100, and the standard deviation is 15, then at least 75% of the values are between 70 and 130. </p>\n\n<p>If the distribution happens to be Normal, then 95% of the values are between 70 and 130. </p>\n\n<p>Generally speaking, IQ test scores are normally distributed and have an average of 100. Someone who is "very bright" is two standard deviations above the mean, meaning an IQ test score of 130.</p>\n', '1995-11-25 15:03:49', '1996-04-27 21:51:48', '412d134d-6bed-125f-1644-cfd00e0a9b3b', '481183cb-5a6c-795b-68f6-423b4e874d0f'),
('2dcedb9c-6c32-7d27-977d-e8c757976496', '<p>It might be useful to explain that "causes" is an asymmetric relation (X causes Y is different from Y causes X), whereas "is correlated with" is a symmetric relation.</p>\n\n<p>For instance, homeless population and crime rate might be correlated, in that both tend to be high or low in the same locations.  It is equally valid to say that homelesss population is correlated with crime rate, or crime rate is correlated with homeless population.  To say that crime causes homelessness, or homeless populations cause crime are different statements.  And correlation does not imply that either is true.  For instance, the underlying cause could be a 3rd variable such as drug abuse, or unemployment.  </p>\n\n<p>The mathematics of statistics is not good at identifying underlying causes, which requires some other form of judgement.</p>\n', '1976-10-19 10:06:44', '1970-01-01 00:00:25', '4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '3738ce93-1661-5628-2cc2-579daad24557'),
('2de001fe-529b-33ad-0280-e8e2a817c80e', '<p>Yes, there are many methods.  You would need to specify which model you''re using, because it can vary.  </p>\n\n<p>For instance, Some models will be compared based on the <a href="http://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a> or <a href="http://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a> criteria.  In other cases, one would look at the <a href="http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">MSE from cross validation</a> (as, for instance, with a support vector machine).</p>\n\n<ol>\n<li>I recommend reading <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a> by Christopher Bishop.</li>\n<li>This is also discussed in Chapter 5 on Credibility, and particularly section 5.5 "Comparing data mining methods" of <a href="http://rads.stackoverflow.com/amzn/click/0120884070">Data Mining: Practical Machine Learning Tools and Techniques</a> by Witten and Frank (which discusses Weka in detail).</li>\n<li>Lastly, you should also have a look at <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a> by Hastie, Tibshirani and Friedman which is available for free online.</li>\n</ol>\n', '1994-07-15 22:38:53', '1970-01-01 00:10:02', '60e226cb-63fb-4363-fc4d-8601bf3d85d8', '595e7d51-318c-39e8-1377-d9ebc53aac3d'),
('2fb0b105-7808-503a-76dd-867949feb8b5', '<p>The first formula is the <em>population</em> standard deviation and the second formula is the the <em>sample</em> standard deviation. The second formula is also related to the unbiased estimator of the variance - see <a href="http://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance">wikipedia</a> for further details.</p>\n\n<p>I suppose (here) in the UK they don''t make the distinction between sample and population at high school. They certainly don''t touch concepts such as biased estimators. </p>\n', '2021-03-25 22:03:10', '1971-07-14 02:44:02', '2ff554db-1bde-573a-701a-bb545470e80c', '7bd9cf39-3f55-3a7b-0035-60b3ebf808a9'),
('30c8ee28-58a3-22f2-5e92-08c864661f9c', '<p><a href="http://data.worldbank.org/data-catalog"><strong>World Bank</strong></a> offers quite a lot of interesting data and has been recently very active in developing nice <a href="http://data.worldbank.org/developers/api-overview">API</a> for it.</p>\n\n<p>Also, <a href="http://www.cs.purdue.edu/commugrate/data_access/all_data_sets.php"><strong>commugrate</strong></a> project has an interesting list available.</p>\n\n<p>For US health related data head for <a href="http://www.healthindicators.gov/"><strong>Health Indicators Warehouse</strong></a>. </p>\n\n<p>Daniel Lemire''s blog <a href="http://lemire.me/blog/archives/2012/03/27/publicly-available-large-data-sets-for-database-research/">points</a> to few interesting examples (mostly tailored towards DB research) including <strong>Canadian Census 1880</strong> and <strong>synoptic cloud reports</strong>.</p>\n\n<p>And as for today (03/04/2012) <a href="http://1940census.archives.gov/"><strong>US 1940 census records</strong></a> are also available to download.</p>\n', '1999-05-08 21:14:55', '2017-04-11 01:46:40', '1e3c96c9-1471-23e0-8765-390639088226', '5c7f0ec4-77f2-3794-cf1d-086412a8ea97'),
('34cde64f-62e4-186c-77dd-867949feb8b5', '<p>In such a discussion, I always recall the famous Ken Thompson quote </p>\n\n<blockquote>\n  <p>When in doubt, use brute force.</p>\n</blockquote>\n\n<p>In this case, machine learning is a salvation when the assumptions are hard to catch; or at least it is much better than guessing them wrong. </p>\n', '1970-01-01 01:00:18', '1996-05-22 10:13:15', '3d6207a1-1e88-4e30-f403-13a65cc92c97', '57cdf8c4-47e3-5560-7e41-c1ec321fe728'),
('3599ef50-13ed-5404-0e63-eeb1d8ed8cef', '<ol>\n<li><p>Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally. Google has made hundreds of billions of dollars not caring about causation. </p></li>\n<li><p>To find causation, you generally need experimental data, not observational data. Though, in economics, they often use observed "shocks" to the system to test for causation, like if a CEO dies suddenly and the stock price goes up, you can assume causation. </p></li>\n<li><p>Correlation is a necessary but not sufficient condition for causation. To show causation requires a counter-factual.</p></li>\n</ol>\n', '1970-01-01 00:01:28', '1982-10-21 23:12:12', '499e3b76-34a8-76c9-1599-04b3a3d51ef2', '12e1b096-3542-127a-1077-d9ebc53aac3d'),
('3c28ae33-ce6a-488c-8d53-e3978efa08ee', '<p><strong>You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales.</strong></p>\n\n<p>Using the correlation matrix <em>standardises</em> the data. In general they give different results. Especially when the scales are different.</p>\n\n<p>As an example, take a look at this R <code>heptathlon</code> data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (run 800m) are around 120.</p>\n\n\n\n<pre class="lang-r prettyprint-override"><code>library(HSAUR)\nheptathlon[,-8]      # look at heptathlon data (excluding ''score'' variable)\n</code></pre>\n\n<p>This outputs:</p>\n\n<pre class="lang-r prettyprint-override"><code>                   hurdles highjump  shot run200m longjump javelin run800m\nJoyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66  128.51\nJohn (GDR)            12.85     1.80 16.23   23.65     6.71   42.56  126.12\nBehmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54  124.20\nSablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78  132.24\nChoubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46  127.90\n...\n</code></pre>\n\n<p>Now let''s do PCA on covariance and on correlation:</p>\n\n<pre class="lang-r prettyprint-override"><code># scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)  \n</code></pre>\n\n<p><a href="http://i.stack.imgur.com/4IwjG.png"><img src="http://i.stack.imgur.com/4IwjG.png" alt="PCA on correlation or covariance"></a></p>\n\n<p>Notice that PCA on covariance is dominated by <code>run800m</code> and <code>javelin</code>: PC1 is almost equal to <code>run800m</code> (and explains $82\\%$ of the variance) and PC2 is almost equal to <code>javelin</code> (together they explain $97\\%$). <strong>PCA on correlation is much more informative</strong> and reveals some structure in the data and relationships between variables (but note that the explained variances drop to $64\\%$ and $71\\%$).</p>\n\n<p>Notice also that the outlying individuals (in <em>this</em> data set) are outliers regardless of whether the covariance or correlation matrix is used.</p>\n', '2021-12-15 12:38:38', '2021-12-15 12:38:38', '18bce813-54a9-6649-f8c3-4d1f326d5ebd', '042272c3-cfff-4dab-81ef-e90b29f3e52a'),
('3f66f2d6-688b-4161-5f92-08c864661f9c', '<p>A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as "state", and then the random variable is a function of the state.</p>\n\n<p>Example:  </p>\n\n<p>Suppose we have three dice rolls ($D_{1}$,$D_{2}$,$D_{3}$).  Then the state $S=(D_{1},D_{2},D_{3})$. </p>\n\n<ol>\n<li>One random variable $X$ is the number of 5s. This is:</li>\n</ol>\n\n<p>$$ X=(D_{1}=5?)+(D_{2}=5?)+(D_{3}=5?)$$</p>\n\n<ol start="2">\n<li>Another random variable $Y$ is the sum of the dice rolls. This is:</li>\n</ol>\n\n<p>$$ Y=D_{1}+D_{2}+D_{3}  $$</p>\n', '1991-09-30 18:19:03', '1993-06-14 21:42:56', '33fad8ab-365a-2815-e078-cf6d4dd5c4a3', '595e7d51-318c-39e8-1377-d9ebc53aac3d'),
('404fe340-1fd3-414b-aadb-e5d54223c257', '<p>Unlike a regular variable, a random variable may not be substituted for a single, unchanging value.  Rather <strong>statistical properties</strong> such as the <strong>distribution</strong> of the random variable may be stated.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.</p>\n\n<p>Random variables may be classified as <em>discrete</em> if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is <em>continuous</em> and is used if the distribution covers values from an uncountable set such as the real numbers.</p>\n', '1972-05-10 14:17:38', '1970-01-01 02:09:09', '60e226cb-63fb-4363-fc4d-8601bf3d85d8', '3e51a46c-4153-229c-7f41-c1ec321fe728'),
('4577565a-7e3e-493a-74dd-867949feb8b5', '<p>When I teach very basic statistics to Secondary School Students I talk about evolution and how we have evolved to spot patterns in pictures rather than lists of numbers and that data visualisation is one of the techniques we use to take advantage of this fact. </p>\n\n<p>Plus I try to talk about recent news stories where statistical insight contradicts what the press is implying, making use of sites like <a href="http://www.gapminder.org/">Gapminder</a> to find the representation before choosing the story.</p>\n', '1990-11-06 17:20:11', '2002-03-21 20:17:11', '609d0ea1-772a-1b4a-8965-390639088226', '11a8748f-3464-740c-28c2-579daad24557'),
('469b3ece-744a-45d5-957d-e8c757976496', '<p>As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous random variable, and the difference for a discrete random variable. </p>\n\n<p>In the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta "functions" can be used to represent these atoms.    </p>\n', '2010-01-14 20:49:04', '2006-11-20 05:18:07', '30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', '4989ff24-23e7-7b82-b6d2-89b22d10517e'),
('46f514e3-3598-177c-6192-08c864661f9c', '<p>Eliciting priors is a tricky business. </p>\n\n<p><a href="http://www.stat.cmu.edu/tr/tr808/tr808.pdf">Statistical Methods for Eliciting Probability Distributions</a> and <a href="http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf">Eliciting Probability Distributions</a> are quite good practical guides for prior elicitation.  The process in both papers is outlined as follows:</p>\n\n<ol>\n<li>background and preparation;</li>\n<li>identifying and recruiting the expert(s);</li>\n<li>motivation and training the expert(s);</li>\n<li>structuring and decomposition (typically deciding precisely what variables should\nbe elicited, and how to elicit joint distributions in the multivariate case);</li>\n<li>the elicitation itself.</li>\n</ol>\n\n<p>Of course, they also review how the elicitation results in information that may be fit to or otherwise define distributions (for instance, in the Bayesian context, <a href="http://en.wikipedia.org/wiki/Beta_distribution">Beta distributions</a>), but quite importantly, they also address common pitfalls in modeling expert knowledge (anchoring, narrow and small-tailed distributions, etc.).</p>\n', '1973-02-07 15:30:51', '1970-01-01 00:05:07', '76598eb0-20b1-664e-8665-390639088226', '481183cb-5a6c-795b-68f6-423b4e874d0f'),
('4760d71f-6e2f-5b32-19cb-66948daf6128', '<p>For me <a href="http://nvac.pnl.gov/agenda.stm">Illuminating the Path</a> report has been always good point of reference.<br>\nFor more recent overview you can also have a look at good <a href="http://queue.acm.org/detail.cfm?id=1805128">article</a> by Heer and colleagues.</p>\n\n<p>But what would explain better than visualization itself?</p>\n\n<p><img src="http://i.stack.imgur.com/uFvje.jpg" alt="alt text"></p>\n\n<p>(<a href="http://blog.ffctn.com/what-is-data-visualization">Source</a>)</p>\n', '2015-04-12 14:20:21', '1970-01-01 00:10:39', '37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '5254e49d-4368-3162-29c2-579daad24557'),
('47f60f67-6087-2a86-abdb-e5d54223c257', '<p>A frequency table is a good place to start. You can do the count, and relative frequency for each level. Also, the total count, and number of missing values may be of use. </p>\n\n<p>You can also use a contingency table to compare two variables at once. Can display using a mosaic plot too.</p>\n', '1995-02-28 06:30:35', '2011-10-22 18:08:06', '749718aa-2270-77cc-1444-cfd00e0a9b3b', '6c80a684-1cb5-41af-ff34-60b3ebf808a9'),
('4d7b21db-2e02-12b7-a9db-e5d54223c257', '<p><a href="http://www.informationisbeautiful.net/" rel="nofollow">Information Is Beautiful</a> | Ideas, issues, knowledge, data - visualized!</p>\n', '2010-01-18 16:11:53', '2006-04-05 03:15:57', '5ef3b4c9-75c9-482e-1699-04b3a3d51ef2', '2afb2146-430d-394b-3d35-0028ed526dd0'),
('4e272fc4-7875-78d6-7d32-6a1673ffca7c', '<p>It depends on where you apply the window function.  If you do it in the time domain, it''s because you only want to analyze the periodic behavior of the function in a short duration.  You do this when you don''t believe that your data is from a stationary process.  </p>\n\n<p>If you do it in the frequency domain, then you do it to isolate a specific set of frequencies for further analysis; you do this when you believe that (for instance) high-frequency components are spurious.</p>\n\n<p>The first three chapters of "A Wavelet Tour of Signal Processing" by Stephane Mallat have an excellent introduction to signal processing in general, and chapter 4 goes into a very good discussion of windowing and time-frequency representations in both continuous and discrete time, along with a few worked-out examples.</p>\n', '1975-10-21 06:56:13', '1970-01-01 00:00:08', '6ed2e962-178c-712c-fd4d-8601bf3d85d8', '3e7dd0ec-7ee5-392a-1277-d9ebc53aac3d'),
('5175a556-3405-4ba6-9f52-18d5e989a512', 'sss', '2021-12-22 01:54:00', '2021-12-22 01:54:00', '3d1c5555-3e10-56cf-2b8f-4d82f3a13455', '40a13723-64bf-409c-a436-6125ec27e2f1'),
('5608f456-3b97-2e05-5d92-08c864661f9c', '<p>In R, the default setting for random number generation are:</p>\n\n<ol>\n<li>For U(0,1), use the Mersenne-Twister algorithm</li>\n<li>For Guassian numbers use  the numerical inversion of the standard normal distribution function. </li>\n</ol>\n\n<p>You can easily check this, viz.</p>\n\n<pre><code>&gt; RNGkind()\n[1] "Mersenne-Twister" "Inversion"\n</code></pre>\n\n<p>It is possible to change the default generator to other PRNGs, such as Super-Duper,Wichmann-Hill, Marsaglia-Multicarry or even a user-supplied PRNG. See the ?RNGkind for further details. I have never needed to change the default PRNG.</p>\n\n<p>The <a href="http://www.gnu.org/software/gsl/manual/html_node/Random-number-environment-variables.html">C GSL</a> library also uses the <a href="http://www.gnu.org/software/gsl/manual/html_node/Random-number-generator-algorithms.html">Mersenne-Twister</a> by default.</p>\n', '1970-01-01 00:05:13', '1970-01-01 00:00:10', '1e3c96c9-1471-23e0-8765-390639088226', '13ede5b6-19dd-563e-1980-de64d090a2c2'),
('5660d465-6020-48f3-75dd-867949feb8b5', '<p>There are many reasons; probably the main is that it works well as parameter of normal distribution.</p>\n', '1987-09-29 14:06:28', '1975-11-29 00:04:24', '11fbda18-5d9f-396c-1299-04b3a3d51ef2', '5f03d745-7702-1c85-2bc2-579daad24557'),
('59d19422-6657-452e-a7db-e5d54223c257', '<p>The squared difference has nicer mathematical properties; it''s continuously differentiable (nice when you want to minimize it), it''s a sufficient statistic for the Gaussian distribution, and it''s (a version of) the L2 norm which comes in handy for proving convergence and so on.</p>\n\n<p>The mean absolute deviation (the absolute value notation you suggest) is also used as a measure of dispersion, but it''s not as "well-behaved" as the squared error.</p>\n', '1970-01-01 00:01:04', '2020-09-12 22:22:31', '3cc58118-77ec-59ed-711a-bb545470e80c', '506acf1e-5331-67a9-0d77-d9ebc53aac3d'),
('5ae2d70b-201d-3e23-7f32-6a1673ffca7c', '<p>Squaring the difference from the mean has a couple of reasons.</p>\n\n<ul>\n<li><p>Variance is defined as the 2nd moment of the deviation (the R.V here is (x-$\\mu$) ) and thus the square as moments are simply the expectations of higher powers of the random variable.</p></li>\n<li><p>Having a square as opposed to the absolute value function gives a nice continuous and differentiable function (absolute value is not differentiable at 0) - which makes it the natural choice, especially in the context of estimation and regression analysis.</p></li>\n<li><p>The squared formulation also naturally falls out of parameters of the Normal Distribution. </p></li>\n</ul>\n', '2019-12-26 12:28:14', '1970-01-01 02:25:46', '37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '197fda97-5a35-781d-0e77-d9ebc53aac3d'),
('5b2fcdf5-2050-7cb2-8f90-67245e8b283e', '<p>My favorite is <a href="http://www.amazon.com/exec/obidos/ISBN=158488388X/">"Bayesian Data Analysis"</a> by Gelman, et al.</p>\n', '1970-01-01 00:00:05', '1970-01-01 00:56:40', '412d134d-6bed-125f-1644-cfd00e0a9b3b', '3304dddb-1b72-607f-25c2-579daad24557'),
('61a45b9e-55c1-3dbb-f917-7000f455f3d5', '<p>Another vote for Gelman et al., but a close second for me -- being of the learn-by-doing persuasion -- is <a href="http://bayes.bgsu.edu/bcwr/">Bayesian Computation with R</a>.</p>\n', '1970-01-01 00:02:22', '2011-12-19 00:47:09', '499e3b76-34a8-76c9-1599-04b3a3d51ef2', '62563348-7bd5-6519-3c35-0028ed526dd0'),
('6429b0f4-1391-7f32-9390-67245e8b283e', '<p>I quite like <a href="http://rads.stackoverflow.com/amzn/click/1584885874" rel="nofollow">Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference</a> by Gamerman and Lopes.</p>\n', '2006-02-06 00:43:32', '2011-12-27 20:36:43', '621899dc-177b-65d2-dbb8-94f47731937d', '409dcd1a-426a-23f7-3a35-0028ed526dd0'),
('645f2e5c-7670-66a0-6092-08c864661f9c', '<p>Let me start by saying that I love both languages: you can''t go wrong with either, and they are certainly better than something like C++ or Java for doing data analysis.</p>\n\n<p>For basic data analysis I would suggest R (especially with plyr).  IMO, R is a little easier to learn than Clojure, although this isn''t completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as <a href="http://mitpress.mit.edu/sicp/">SICP</a>).  There are less keywords in Clojure, but the libraries are much more difficult to install and work with.  Also, keep in mind that R (or S) is largely derived from Scheme, so you would benefit from Lisp knowledge when using it.</p>\n\n<p>In general:</p>\n\n<p>The main advantage of R is the community on CRAN (over 2461  packages and counting).  Nothing will compare with this in the near future, not even a commercial application like matlab.</p>\n\n<p>Clojure has the big advantage of running on the JVM which means that it can use any Java based library immediately.</p>\n\n<p>I would add that I gave <a href="http://bit.ly/dhDZkp">a talk relating Clojure/Incanter to R</a> a while ago, so you may find it of interest.  In my experience around creating this, Clojure was generally slower than R for simple operations.</p>\n', '1977-03-16 01:39:21', '1970-01-01 00:00:13', '76598eb0-20b1-664e-8665-390639088226', '5c7f0ec4-77f2-3794-cf1d-086412a8ea97'),
('6764ab86-6fcc-2757-8032-6a1673ffca7c', '<p>Coming from non-statistical background I found <a href="http://www.princeton.edu/~slynch/bayesbook/bookinfo.html">Introduction to Applied Bayesian Statistics and Estimation for Social Scientists</a> quite informative and easy to follow.</p>\n', '1991-12-14 04:16:06', '1985-01-03 20:50:12', '4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', '11a8748f-3464-740c-28c2-579daad24557'),
('68478414-705a-2626-5c92-08c864661f9c', '<p>I believe that this calls for a <a href="http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm">two-sample KolmogorovÃ¢Â€Â“Smirnov test</a>, or the like.  The two-sample KolmogorovÃ¢Â€Â“Smirnov test is based on comparing differences in the <a href="http://en.wikipedia.org/wiki/Empirical_distribution_function">empirical distribution functions</a> (ECDF) of two samples, meaning it is sensitive to both location and shape of the the two samples.  It also generalizes out to a multivariate form.</p>\n\n<p>This test is found in various forms in different packages in R, so if you are basically proficient, all you have to do is install one of them (e.g. <a href="http://cran.r-project.org/web/packages/fBasics/fBasics.pdf">fBasics</a>), and run it on your sample data.</p>\n', '1970-01-01 00:00:01', '1970-01-01 00:01:06', '5ada22d3-3248-6596-6e1a-bb545470e80c', '197fda97-5a35-781d-0e77-d9ebc53aac3d'),
('6916f8cf-1279-16d8-9290-67245e8b283e', '<p>I recommend these books - they are highly rated on Amazon too:</p>\n\n<p>"Text Mining" by Weiss</p>\n\n<p>"Text Mining Application Programming", by Konchady</p>\n\n<p>For software, I recommend RapidMiner (with the text plugin), free and open-source.</p>\n\n<p>This is my "text mining process":</p>\n\n<pre><code>* collect the documents (usually a web crawl)\n      o [sample if too large]\n      o timestamp\n      o strip out markup\n* tokenize: break into characters, words, n-grams, or sliding windows\n* stemming (aka lemmatization)\n      o [include synonyms]\n      o see porter or snowflake algorithm\n      o pronouns and articles are usually bad predictors\n* remove stopwords\n* feature vectorization\n      o binary (appears or doesnÃ¢Â€Â™t)\n      o word count\n      o relative frequency: tf-idf\n      o information gain, chi square\n      o [have a minimum value for inclusion]\n* weighting\n      o weight words at top of document higher?\n</code></pre>\n\n<p>Then you can start the work of classifying them. kNN, SVM, or Naive Bayes as appropriate. </p>\n\n<p>You can see my series of text mining videos <a href="http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-loading.html" rel="nofollow">here</a></p>\n', '1998-12-04 21:53:12', '1981-01-21 02:37:44', '1876fb44-59e4-7596-dcb8-94f47731937d', '12693b29-274c-35ce-d11d-086412a8ea97'),
('69cdb3c5-55bc-2a7a-9a7d-e8c757976496', '<p>Neural network may be to slow for a large number of documents (also this is now pretty much obsolete).<br>\nAnd you may also check Random Forest among classifiers; it is quite fast, scales nice and does not need complex tuning.</p>\n', '2008-02-07 02:45:25', '1995-02-03 04:48:15', '15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', '409dcd1a-426a-23f7-3a35-0028ed526dd0'),
('6a4d99cd-242a-6200-7add-867949feb8b5', '<p>A principal component is a weighted linear combination of all your factors (X''s).</p>\n\n<p>example: PC1 = 0.1X1 + 0.3X2</p>\n\n<p>There will be one component for each factor (though in general a small number are selected). </p>\n\n<p>The components are created such that they have zero correlation (are orthogonal), by design.</p>\n\n<p>Therefore, component PC1 should not explain any variation in component PC2.</p>\n\n<p>You may want to do regression on your Y variable and the PCA representation of your X''s, as they will not have multi-collinearity. However, this could be hard to interpret.</p>\n\n<p>If you have more X''s than observations, which breaks OLS, you can regress on your components, and simply select a smaller number of the highest variation components. </p>\n\n<p><a href="http://rads.stackoverflow.com/amzn/click/0387954422" rel="nofollow">Principal Component Analysis</a> by Jollife a very in-depth and highly cited book on the subject</p>\n\n<p>This is also good: <a href="http://www.statsoft.com/textbook/principal-components-factor-analysis/" rel="nofollow">http://www.statsoft.com/textbook/principal-components-factor-analysis/</a></p>\n', '1996-02-08 10:06:25', '1970-01-01 00:07:20', '621899dc-177b-65d2-dbb8-94f47731937d', '3fed64ec-2f94-798f-0335-60b3ebf808a9'),
('7623d42f-35f4-1ce5-987d-e8c757976496', '<ol>\n<li><p>If you carved your distribution (histogram) out\nof wood, and tried to balance it on\nyour finger, the balance point would\nbe the mean, no matter the shape of the distribution.</p></li>\n<li><p>If you put a stick in the middle of\nyour scatter plot, and attached the\nstick to each data point with a\nspring, the resting point of the\nstick would be your regression line. [1]</p></li>\n</ol>\n\n<p>[1] this would technically be principal components regression. you would have to force the springs to move only "vertically" to be least squares, but the example is illustrative either way. </p>\n', '2011-10-07 05:37:06', '1997-05-17 14:51:50', '4b635464-5ee2-2a80-f103-13a65cc92c97', '5fab4ff7-1b8e-64d0-1880-de64d090a2c2'),
('7686595d-16d5-33b3-0080-e8e2a817c80e', '<p>Sample size doesn''t much depend on the population size, which is counter-intuitive to many.</p>\n\n<p>Most polling companies use 400 or 1000 people in their samples.</p>\n\n<p>There is a reason for this:</p>\n\n<p>A sample size of 400 will give you a confidence interval of +/-5% 19 times out of 20 (95%)</p>\n\n<p>A sample size of 1000 will give you a confidence interval of +/-3% 19 times out of 20 (95%)</p>\n\n<p>When you are measuring a proportion near 50% anyways. </p>\n\n<p>This calculator isn''t bad:</p>\n\n<p><a href="http://www.raosoft.com/samplesize.html">http://www.raosoft.com/samplesize.html</a></p>\n', '1980-10-16 21:34:15', '1995-09-22 02:15:40', '6d81a322-63f0-7270-6d1a-bb545470e80c', '12693b29-274c-35ce-d11d-086412a8ea97'),
('768f8e64-7d10-20c9-967d-e8c757976496', '<p><a href="http://rapid-i.com/" rel="nofollow">RapidMiner</a> for data and text mining</p>\n', '1982-06-22 16:35:52', '1987-05-16 14:05:24', '499e3b76-34a8-76c9-1599-04b3a3d51ef2', '595e7d51-318c-39e8-1377-d9ebc53aac3d'),
('771e3dd1-42bb-2339-acdb-e5d54223c257', '<p>Sometimes outliers are bad data, and should be excluded, such as typos. Sometimes they are Wayne Gretzky or Michael Jordan, and should be kept. </p>\n\n<p>Outlier detection methods include:</p>\n\n<p>Univariate -> boxplot. outside of 1.5 times inter-quartile range is an outlier.</p>\n\n<p>Bivariate -> scatterplot with confidence ellipse. outside of, say, 95% confidence ellipse is an outlier. </p>\n\n<p>Multivariate -> Mahalanobis D2 distance</p>\n\n<p>Mark those observations as outliers.</p>\n\n<p>Run a logistic regression (on Y=IsOutlier) to see if there are any systematic patterns. </p>\n\n<p>Remove ones that you can demonstrate they are not representative of any sub-population. </p>\n', '2018-01-21 08:07:06', '1970-01-01 01:59:55', '5e272913-6cff-509f-d8b8-94f47731937d', '13ede5b6-19dd-563e-1980-de64d090a2c2'),
('7bec5682-21c4-666f-a8db-e5d54223c257', 'Excepturi debitis perspiciatis ea.', '1975-10-30 20:43:30', '1970-01-01 00:06:55', '11643064-22fb-7e14-1399-04b3a3d51ef2', '6f51ea57-13b2-139f-1177-d9ebc53aac3d'),
('8acd0518-13ab-4556-8853-678921f51367', 'aaaa', '2021-12-15 12:57:26', '2021-12-15 12:57:33', '18bce813-54a9-6649-f8c3-4d1f326d5ebd', '481183cb-5a6c-795b-68f6-423b4e874d0f'),
('8fa157d4-fb32-4bdb-b170-212b6f1de902', 'o', '2021-12-22 01:34:25', '2021-12-22 01:34:25', '3d1c5555-3e10-56cf-2b8f-4d82f3a13455', '40a13723-64bf-409c-a436-6125ec27e2f1'),
('9e7f8a40-44bd-4692-9653-0cdcc1e68563', 'sssas', '2021-12-15 12:54:56', '2021-12-15 12:54:56', '18bce813-54a9-6649-f8c3-4d1f326d5ebd', '042272c3-cfff-4dab-81ef-e90b29f3e52a'),
('adc098b0-d267-4467-969e-d9078bf46cd6', 'ádasdasd', '2021-12-22 14:04:39', '2021-12-22 14:04:39', '3cc58118-77ec-59ed-711a-bb545470e80c', '35c3a053-254d-33f3-1b80-de64d090a2c2'),
('c035e0b3-52d6-4cc4-b808-2b5fd6494a42', 'asdassas', '2021-12-15 00:32:19', '2021-12-15 00:32:19', '18bce813-54a9-6649-f8c3-4d1f326d5ebd', 'b2f86f84-7d9d-48aa-954e-879a723d3732'),
('d2307a83-4150-4fa9-a19a-be406be67cc9', 'Ngo The @k', '2021-12-16 22:02:12', '2021-12-16 22:02:28', '2ff554db-1bde-573a-701a-bb545470e80c', '481183cb-5a6c-795b-68f6-423b4e874d0f'),
('d58422da-adad-4ba5-8447-8a66fb86d724', 'thetan', '2021-12-15 18:09:03', '2021-12-15 18:09:03', '5bc250e9-354c-43e2-a091-9e1ee725216c', '6c80a684-1cb5-41af-ff34-60b3ebf808a9'),
('dd90ea92-be7e-4c9b-8431-e20873809161', 'ádasdasdas', '2021-12-23 22:11:15', '2021-12-23 22:11:21', '145aebb1-a59e-4f22-93eb-cacd263eaf3f', '7b666cc3-1b67-6037-67f6-423b4e874d0f');

-- 
-- Dumping data for table tags
--
INSERT INTO tags VALUES
('148ed882-32b8-218e-9c20-39c2f00615e8', 'Javascript'),
('1b691e79-236d-5b5a-9d20-39c2f00615e8', 'CSS'),
('25c6c36e-1668-7d10-6e09-bf1378b8dc91', 'C#'),
('354f1b13-17bf-1b52-87d5-ba100c6f7bce', 'Java'),
('3700cc49-55b5-69ea-4929-a2925c0f334d', 'Ktolin'),
('37dd9bb0-4c53-4134-31de-23e177779933', 'HTML'),
('3b86d2ed-446c-5fce-56be-406293204378', 'Python'),
('6b47b37f-3123-3ce7-14cf-9712082ff6cb', 'C');

-- 
-- Dumping data for table usertag
--
INSERT INTO usertag VALUES
('412d134d-6bed-125f-1644-cfd00e0a9b3b', '3b86d2ed-446c-5fce-56be-406293204378'),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('15f49a4f-6117-2234-1544-cfd00e0a9b3b', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('3cc58118-77ec-59ed-711a-bb545470e80c', '37dd9bb0-4c53-4134-31de-23e177779933'),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('7ac8b8cf-6a0d-6fa3-d9b8-94f47731937d', '3b86d2ed-446c-5fce-56be-406293204378'),
('697be7ba-1738-6adf-298f-4d82f3a13455', '37dd9bb0-4c53-4134-31de-23e177779933'),
('7de0e887-6ace-4435-1344-cfd00e0a9b3b', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('5e272913-6cff-509f-d8b8-94f47731937d', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '3b86d2ed-446c-5fce-56be-406293204378'),
('4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('33fad8ab-365a-2815-e078-cf6d4dd5c4a3', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('1876fb44-59e4-7596-dcb8-94f47731937d', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('15f49a4f-6117-2234-1544-cfd00e0a9b3b', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('3cc58118-77ec-59ed-711a-bb545470e80c', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('5ef3b4c9-75c9-482e-1699-04b3a3d51ef2', '37dd9bb0-4c53-4134-31de-23e177779933'),
('609d0ea1-772a-1b4a-8965-390639088226', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('6f2979d3-58ec-3673-f303-13a65cc92c97', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('3d1c5555-3e10-56cf-2b8f-4d82f3a13455', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('7de0e887-6ace-4435-1344-cfd00e0a9b3b', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('68069d95-76b4-1dce-f503-13a65cc92c97', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('3cc58118-77ec-59ed-711a-bb545470e80c', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('7ac8b8cf-6a0d-6fa3-d9b8-94f47731937d', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('1beedfca-2f14-1d94-8865-390639088226', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('5bfe5648-65c2-7d1a-f203-13a65cc92c97', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('3d6207a1-1e88-4e30-f403-13a65cc92c97', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('5bfe5648-65c2-7d1a-f203-13a65cc92c97', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('694c75bd-3b3d-2034-6f1a-bb545470e80c', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('412d134d-6bed-125f-1644-cfd00e0a9b3b', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('3d1c5555-3e10-56cf-2b8f-4d82f3a13455', '3b86d2ed-446c-5fce-56be-406293204378'),
('1beedfca-2f14-1d94-8865-390639088226', '3b86d2ed-446c-5fce-56be-406293204378'),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('33c6f0f4-3591-2c8e-1244-cfd00e0a9b3b', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('694c75bd-3b3d-2034-6f1a-bb545470e80c', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('5ada22d3-3248-6596-6e1a-bb545470e80c', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('6d81a322-63f0-7270-6d1a-bb545470e80c', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('76753dc8-4972-2ecd-2e8f-4d82f3a13455', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('1beedfca-2f14-1d94-8865-390639088226', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('11643064-22fb-7e14-1399-04b3a3d51ef2', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('5ef3b4c9-75c9-482e-1699-04b3a3d51ef2', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('33fad8ab-365a-2815-e078-cf6d4dd5c4a3', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('3d1c5555-3e10-56cf-2b8f-4d82f3a13455', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('5ada22d3-3248-6596-6e1a-bb545470e80c', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('499e3b76-34a8-76c9-1599-04b3a3d51ef2', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('697be7ba-1738-6adf-298f-4d82f3a13455', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('4b635464-5ee2-2a80-f103-13a65cc92c97', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('2e41b1ec-1c05-1bc6-dab8-94f47731937d', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('6f2979d3-58ec-3673-f303-13a65cc92c97', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('1e3c96c9-1471-23e0-8765-390639088226', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('60e226cb-63fb-4363-fc4d-8601bf3d85d8', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('11643064-22fb-7e14-1399-04b3a3d51ef2', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('4b635464-5ee2-2a80-f103-13a65cc92c97', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '3b86d2ed-446c-5fce-56be-406293204378'),
('694c75bd-3b3d-2034-6f1a-bb545470e80c', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('76598eb0-20b1-664e-8665-390639088226', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('6ed2e962-178c-712c-fd4d-8601bf3d85d8', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('7ac8b8cf-6a0d-6fa3-d9b8-94f47731937d', '3b86d2ed-446c-5fce-56be-406293204378'),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '37dd9bb0-4c53-4134-31de-23e177779933'),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '3b86d2ed-446c-5fce-56be-406293204378'),
('1fee0bc4-513a-2c25-004e-8601bf3d85d8', '3b86d2ed-446c-5fce-56be-406293204378'),
('5ada22d3-3248-6596-6e1a-bb545470e80c', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('5e272913-6cff-509f-d8b8-94f47731937d', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('76753dc8-4972-2ecd-2e8f-4d82f3a13455', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('75de26bb-5a5c-467f-2c8f-4d82f3a13455', '3b86d2ed-446c-5fce-56be-406293204378'),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('609d0ea1-772a-1b4a-8965-390639088226', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('694c75bd-3b3d-2034-6f1a-bb545470e80c', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('4b635464-5ee2-2a80-f103-13a65cc92c97', '3b86d2ed-446c-5fce-56be-406293204378'),
('18bce813-54a9-6649-f8c3-4d1f326d5ebd', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('18bce813-54a9-6649-f8c3-4d1f326d5ebd', '3b86d2ed-446c-5fce-56be-406293204378'),
('5bc250e9-354c-43e2-a091-9e1ee725216c', '37dd9bb0-4c53-4134-31de-23e177779933'),
('5bc250e9-354c-43e2-a091-9e1ee725216c', '3b86d2ed-446c-5fce-56be-406293204378'),
('5bc250e9-354c-43e2-a091-9e1ee725216c', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('5bc250e9-354c-43e2-a091-9e1ee725216c', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('2ff554db-1bde-573a-701a-bb545470e80c', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('2ff554db-1bde-573a-701a-bb545470e80c', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('2ff554db-1bde-573a-701a-bb545470e80c', '3b86d2ed-446c-5fce-56be-406293204378'),
('2ff554db-1bde-573a-701a-bb545470e80c', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '37dd9bb0-4c53-4134-31de-23e177779933'),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '3b86d2ed-446c-5fce-56be-406293204378'),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '6b47b37f-3123-3ce7-14cf-9712082ff6cb');

-- 
-- Dumping data for table userquestion
--
INSERT INTO userquestion VALUES
('694c75bd-3b3d-2034-6f1a-bb545470e80c', '506acf1e-5331-67a9-0d77-d9ebc53aac3d', 1),
('621899dc-177b-65d2-dbb8-94f47731937d', '6b550b8f-7ff9-51bc-0f77-d9ebc53aac3d', 1),
('76753dc8-4972-2ecd-2e8f-4d82f3a13455', '42b9446a-790b-3892-2ac2-579daad24557', -1),
('3cc58118-77ec-59ed-711a-bb545470e80c', '7b666cc3-1b67-6037-67f6-423b4e874d0f', 1),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '56aaf263-3355-23ea-b8d2-89b22d10517e', -1),
('6207a9a7-3bf9-7288-8565-390639088226', '5254e49d-4368-3162-29c2-579daad24557', -1),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '5254e49d-4368-3162-29c2-579daad24557', -1),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '3fed64ec-2f94-798f-0335-60b3ebf808a9', 1),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '5254e49d-4368-3162-29c2-579daad24557', 1),
('33fad8ab-365a-2815-e078-cf6d4dd5c4a3', '407db4d1-222b-287d-0135-60b3ebf808a9', -1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '62563348-7bd5-6519-3c35-0028ed526dd0', 1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '524751f2-1577-523d-ce1d-086412a8ea97', -1),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', '409dcd1a-426a-23f7-3a35-0028ed526dd0', 1),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '5254e49d-4368-3162-29c2-579daad24557', 1),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '524751f2-1577-523d-ce1d-086412a8ea97', 1),
('15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', '28cd1599-63bf-785f-4f80-7ed9d1ea06ea', -1),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '4989ff24-23e7-7b82-b6d2-89b22d10517e', -1),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '6c80a684-1cb5-41af-ff34-60b3ebf808a9', -1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '4989ff24-23e7-7b82-b6d2-89b22d10517e', -1),
('4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '57cdf8c4-47e3-5560-7e41-c1ec321fe728', -1),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '5254e49d-4368-3162-29c2-579daad24557', 1),
('1876fb44-59e4-7596-dcb8-94f47731937d', '2afb2146-430d-394b-3d35-0028ed526dd0', -1),
('4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', '4db9a514-6d87-2ec1-3b35-0028ed526dd0', 1),
('1fee0bc4-513a-2c25-004e-8601bf3d85d8', '28cd1599-63bf-785f-4f80-7ed9d1ea06ea', 1),
('621899dc-177b-65d2-dbb8-94f47731937d', '5254e49d-4368-3162-29c2-579daad24557', 1),
('2e41b1ec-1c05-1bc6-dab8-94f47731937d', '2f43fc9d-1f7a-25fc-5280-7ed9d1ea06ea', -1),
('2e41b1ec-1c05-1bc6-dab8-94f47731937d', '7ad34860-1ac8-7ad7-d01d-086412a8ea97', -1),
('2e41b1ec-1c05-1bc6-dab8-94f47731937d', '5254e49d-4368-3162-29c2-579daad24557', 1),
('7de0e887-6ace-4435-1344-cfd00e0a9b3b', '12693b29-274c-35ce-d11d-086412a8ea97', 1),
('3d6207a1-1e88-4e30-f403-13a65cc92c97', '7ad34860-1ac8-7ad7-d01d-086412a8ea97', -1),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '5254e49d-4368-3162-29c2-579daad24557', 1),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '677132b4-2a33-68d1-26c2-579daad24557', -1),
('499e3b76-34a8-76c9-1599-04b3a3d51ef2', '35c3a053-254d-33f3-1b80-de64d090a2c2', -1),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '11a8748f-3464-740c-28c2-579daad24557', -1),
('6ed2e962-178c-712c-fd4d-8601bf3d85d8', '56dce332-76f0-6b9b-5080-7ed9d1ea06ea', 1),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '1e5e96fa-5d7a-599d-27c2-579daad24557', -1),
('697be7ba-1738-6adf-298f-4d82f3a13455', '73dc79e1-1ccb-35a6-0235-60b3ebf808a9', 1),
('412d134d-6bed-125f-1644-cfd00e0a9b3b', '677132b4-2a33-68d1-26c2-579daad24557', 1),
('3d6207a1-1e88-4e30-f403-13a65cc92c97', '6f51ea57-13b2-139f-1177-d9ebc53aac3d', -1),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '481183cb-5a6c-795b-68f6-423b4e874d0f', 1),
('697be7ba-1738-6adf-298f-4d82f3a13455', '62af2231-78a7-7caa-cd1d-086412a8ea97', 1),
('33fad8ab-365a-2815-e078-cf6d4dd5c4a3', '524751f2-1577-523d-ce1d-086412a8ea97', -1),
('1beedfca-2f14-1d94-8865-390639088226', '595e7d51-318c-39e8-1377-d9ebc53aac3d', 1),
('30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', '73dc79e1-1ccb-35a6-0235-60b3ebf808a9', 1),
('2ff554db-1bde-573a-701a-bb545470e80c', '3738ce93-1661-5628-2cc2-579daad24557', 1),
('5ef3b4c9-75c9-482e-1699-04b3a3d51ef2', '57cdf8c4-47e3-5560-7e41-c1ec321fe728', 1),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '3e51a46c-4153-229c-7f41-c1ec321fe728', -1),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', '3e51a46c-4153-229c-7f41-c1ec321fe728', 1),
('1beedfca-2f14-1d94-8865-390639088226', '481183cb-5a6c-795b-68f6-423b4e874d0f', -1),
('5ef3b4c9-75c9-482e-1699-04b3a3d51ef2', '6485b64f-7835-65a4-8041-c1ec321fe728', -1),
('609d0ea1-772a-1b4a-8965-390639088226', '5c7f0ec4-77f2-3794-cf1d-086412a8ea97', -1),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '197fda97-5a35-781d-0e77-d9ebc53aac3d', 1),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '407db4d1-222b-287d-0135-60b3ebf808a9', -1),
('7de0e887-6ace-4435-1344-cfd00e0a9b3b', '5fab4ff7-1b8e-64d0-1880-de64d090a2c2', -1),
('6f2979d3-58ec-3673-f303-13a65cc92c97', '13ede5b6-19dd-563e-1980-de64d090a2c2', 1),
('412d134d-6bed-125f-1644-cfd00e0a9b3b', '56dce332-76f0-6b9b-5080-7ed9d1ea06ea', -1),
('1fee0bc4-513a-2c25-004e-8601bf3d85d8', '3e51a46c-4153-229c-7f41-c1ec321fe728', 1),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '2afb2146-430d-394b-3d35-0028ed526dd0', 1),
('15f49a4f-6117-2234-1544-cfd00e0a9b3b', '197fda97-5a35-781d-0e77-d9ebc53aac3d', -1),
('1876fb44-59e4-7596-dcb8-94f47731937d', '6b550b8f-7ff9-51bc-0f77-d9ebc53aac3d', 1),
('33c6f0f4-3591-2c8e-1244-cfd00e0a9b3b', '4989ff24-23e7-7b82-b6d2-89b22d10517e', 1),
('76753dc8-4972-2ecd-2e8f-4d82f3a13455', '11a8748f-3464-740c-28c2-579daad24557', 1),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '1e5e96fa-5d7a-599d-27c2-579daad24557', 1),
('15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', '28cd1599-63bf-785f-4f80-7ed9d1ea06ea', 1),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '3304dddb-1b72-607f-25c2-579daad24557', 1),
('75de26bb-5a5c-467f-2c8f-4d82f3a13455', '57cdf8c4-47e3-5560-7e41-c1ec321fe728', -1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '12693b29-274c-35ce-d11d-086412a8ea97', -1),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '6b550b8f-7ff9-51bc-0f77-d9ebc53aac3d', 1),
('15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', '2afb2146-430d-394b-3d35-0028ed526dd0', 1),
('6ed2e962-178c-712c-fd4d-8601bf3d85d8', '4989ff24-23e7-7b82-b6d2-89b22d10517e', -1),
('697be7ba-1738-6adf-298f-4d82f3a13455', '42b9446a-790b-3892-2ac2-579daad24557', -1),
('5e272913-6cff-509f-d8b8-94f47731937d', '6f51ea57-13b2-139f-1177-d9ebc53aac3d', -1),
('33c6f0f4-3591-2c8e-1244-cfd00e0a9b3b', '524751f2-1577-523d-ce1d-086412a8ea97', 1),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '5254e49d-4368-3162-29c2-579daad24557', -1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '12e1b096-3542-127a-1077-d9ebc53aac3d', -1),
('76753dc8-4972-2ecd-2e8f-4d82f3a13455', '481183cb-5a6c-795b-68f6-423b4e874d0f', 1),
('499e3b76-34a8-76c9-1599-04b3a3d51ef2', '12e1b096-3542-127a-1077-d9ebc53aac3d', 1),
('1fee0bc4-513a-2c25-004e-8601bf3d85d8', '62af2231-78a7-7caa-cd1d-086412a8ea97', 1),
('4b635464-5ee2-2a80-f103-13a65cc92c97', '11a8748f-3464-740c-28c2-579daad24557', 1),
('15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', '3738ce93-1661-5628-2cc2-579daad24557', -1),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '3e7dd0ec-7ee5-392a-1277-d9ebc53aac3d', 1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '1e5e96fa-5d7a-599d-27c2-579daad24557', 1),
('6ed2e962-178c-712c-fd4d-8601bf3d85d8', '4db9a514-6d87-2ec1-3b35-0028ed526dd0', -1),
('30d623f2-31cf-5d4c-f7c3-4d1f326d5ebd', '407db4d1-222b-287d-0135-60b3ebf808a9', 1),
('7ac8b8cf-6a0d-6fa3-d9b8-94f47731937d', '62af2231-78a7-7caa-cd1d-086412a8ea97', 1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '3e7dd0ec-7ee5-392a-1277-d9ebc53aac3d', -1),
('4bfe55ba-57df-274c-1144-cfd00e0a9b3b', '6485b64f-7835-65a4-8041-c1ec321fe728', 1),
('1e3c96c9-1471-23e0-8765-390639088226', '62af2231-78a7-7caa-cd1d-086412a8ea97', 1),
('4a0e751e-750e-45c3-de78-cf6d4dd5c4a3', '2afb2146-430d-394b-3d35-0028ed526dd0', -1),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '6c80a684-1cb5-41af-ff34-60b3ebf808a9', -1),
('1876fb44-59e4-7596-dcb8-94f47731937d', '36ac2df8-2b1e-368f-cc1d-086412a8ea97', -1),
('697be7ba-1738-6adf-298f-4d82f3a13455', '595e7d51-318c-39e8-1377-d9ebc53aac3d', -1),
('11fbda18-5d9f-396c-1299-04b3a3d51ef2', '5254e49d-4368-3162-29c2-579daad24557', 1),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '56dce332-76f0-6b9b-5080-7ed9d1ea06ea', -1),
('1e3c96c9-1471-23e0-8765-390639088226', '5254e49d-4368-3162-29c2-579daad24557', -1),
('1beedfca-2f14-1d94-8865-390639088226', '73dc79e1-1ccb-35a6-0235-60b3ebf808a9', 1),
('15b4ab0a-3b8f-6f6f-2a8f-4d82f3a13455', '35c3a053-254d-33f3-1b80-de64d090a2c2', 1),
('75de26bb-5a5c-467f-2c8f-4d82f3a13455', '197fda97-5a35-781d-0e77-d9ebc53aac3d', -1),
('33fad8ab-365a-2815-e078-cf6d4dd5c4a3', '506acf1e-5331-67a9-0d77-d9ebc53aac3d', 1),
('3cc58118-77ec-59ed-711a-bb545470e80c', '57cdf8c4-47e3-5560-7e41-c1ec321fe728', 1),
('2ff554db-1bde-573a-701a-bb545470e80c', '481183cb-5a6c-795b-68f6-423b4e874d0f', -1),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '5f03d745-7702-1c85-2bc2-579daad24557', 1),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '407db4d1-222b-287d-0135-60b3ebf808a9', -1),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '52f3ff99-7fff-436f-9404-c27ed4e2a482', 1);

-- 
-- Dumping data for table useranswer
--
INSERT INTO useranswer VALUES
('4b635464-5ee2-2a80-f103-13a65cc92c97', '284ccb05-337b-7e6f-9090-67245e8b283e', -1),
('4b635464-5ee2-2a80-f103-13a65cc92c97', '142cb08f-7c31-21fa-8e90-67245e8b283e', -1),
('33c6f0f4-3591-2c8e-1244-cfd00e0a9b3b', '1f8d7e46-1d1e-335f-79dd-867949feb8b5', 1),
('6207a9a7-3bf9-7288-8565-390639088226', '4760d71f-6e2f-5b32-19cb-66948daf6128', 1),
('694c75bd-3b3d-2034-6f1a-bb545470e80c', '47f60f67-6087-2a86-abdb-e5d54223c257', -1),
('694c75bd-3b3d-2034-6f1a-bb545470e80c', '768f8e64-7d10-20c9-967d-e8c757976496', 1),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '1ded3dd7-32db-5bc9-9190-67245e8b283e', -1),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', '1ded3dd7-32db-5bc9-9190-67245e8b283e', 1),
('4e53ab23-4fbb-7431-fe4d-8601bf3d85d8', '11f7089b-33e6-371a-f817-7000f455f3d5', 1),
('11643064-22fb-7e14-1399-04b3a3d51ef2', '30c8ee28-58a3-22f2-5e92-08c864661f9c', 1),
('609d0ea1-772a-1b4a-8965-390639088226', '2bbbd485-6df2-536a-7e32-6a1673ffca7c', 1),
('1beedfca-2f14-1d94-8865-390639088226', '5660d465-6020-48f3-75dd-867949feb8b5', -1),
('1876fb44-59e4-7596-dcb8-94f47731937d', '768f8e64-7d10-20c9-967d-e8c757976496', 1),
('5bfe5648-65c2-7d1a-f203-13a65cc92c97', '3f66f2d6-688b-4161-5f92-08c864661f9c', -1),
('5bfe5648-65c2-7d1a-f203-13a65cc92c97', '2dcedb9c-6c32-7d27-977d-e8c757976496', -1),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '1f74cd3f-6de4-5324-0180-e8e2a817c80e', 1),
('609d0ea1-772a-1b4a-8965-390639088226', '645f2e5c-7670-66a0-6092-08c864661f9c', 1),
('76598eb0-20b1-664e-8665-390639088226', '7686595d-16d5-33b3-0080-e8e2a817c80e', -1),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '3599ef50-13ed-5404-0e63-eeb1d8ed8cef', -1),
('4715a96a-6e8c-2acb-1499-04b3a3d51ef2', '34cde64f-62e4-186c-77dd-867949feb8b5', 1),
('18bce813-54a9-6649-f8c3-4d1f326d5ebd', '6764ab86-6fcc-2757-8032-6a1673ffca7c', 1),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '61a45b9e-55c1-3dbb-f917-7000f455f3d5', -1),
('6207a9a7-3bf9-7288-8565-390639088226', '7bec5682-21c4-666f-a8db-e5d54223c257', 1),
('5ada22d3-3248-6596-6e1a-bb545470e80c', '61a45b9e-55c1-3dbb-f917-7000f455f3d5', 1),
('7de0e887-6ace-4435-1344-cfd00e0a9b3b', '2bbbd485-6df2-536a-7e32-6a1673ffca7c', 1),
('60e226cb-63fb-4363-fc4d-8601bf3d85d8', '2b850d3d-6722-3f01-1bcb-66948daf6128', -1),
('2ff554db-1bde-573a-701a-bb545470e80c', '768f8e64-7d10-20c9-967d-e8c757976496', -1),
('6ed2e962-178c-712c-fd4d-8601bf3d85d8', '34cde64f-62e4-186c-77dd-867949feb8b5', -1),
('5e6695e2-5aad-3241-6c1a-bb545470e80c', '3599ef50-13ed-5404-0e63-eeb1d8ed8cef', -1),
('7de0e887-6ace-4435-1344-cfd00e0a9b3b', '5608f456-3b97-2e05-5d92-08c864661f9c', 1),
('4e3e50b0-55de-5c2f-2d8f-4d82f3a13455', '1ded3dd7-32db-5bc9-9190-67245e8b283e', -1),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '4760d71f-6e2f-5b32-19cb-66948daf6128', 1),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '7686595d-16d5-33b3-0080-e8e2a817c80e', -1),
('2ff554db-1bde-573a-701a-bb545470e80c', '7686595d-16d5-33b3-0080-e8e2a817c80e', 1),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '6764ab86-6fcc-2757-8032-6a1673ffca7c', -1),
('6f10885a-526e-7c3d-2f8f-4d82f3a13455', '3599ef50-13ed-5404-0e63-eeb1d8ed8cef', 1),
('3d6207a1-1e88-4e30-f403-13a65cc92c97', '11452b0c-768e-5ff7-0d63-eeb1d8ed8cef', -1),
('18bce813-54a9-6649-f8c3-4d1f326d5ebd', '1ded3dd7-32db-5bc9-9190-67245e8b283e', 1),
('499e3b76-34a8-76c9-1599-04b3a3d51ef2', '284ccb05-337b-7e6f-9090-67245e8b283e', -1),
('749718aa-2270-77cc-1444-cfd00e0a9b3b', '768f8e64-7d10-20c9-967d-e8c757976496', -1),
('68069d95-76b4-1dce-f503-13a65cc92c97', '17120d02-6ab5-3e43-18cb-66948daf6128', -1),
('5bfe5648-65c2-7d1a-f203-13a65cc92c97', '6764ab86-6fcc-2757-8032-6a1673ffca7c', -1),
('5e272913-6cff-509f-d8b8-94f47731937d', '205a3e0a-13fe-3557-8232-6a1673ffca7c', 1),
('1beedfca-2f14-1d94-8865-390639088226', '14c0ce35-1687-459c-1acb-66948daf6128', -1),
('609d0ea1-772a-1b4a-8965-390639088226', '4577565a-7e3e-493a-74dd-867949feb8b5', 1),
('621899dc-177b-65d2-dbb8-94f47731937d', '284ccb05-337b-7e6f-9090-67245e8b283e', -1),
('609d0ea1-772a-1b4a-8965-390639088226', '11452b0c-768e-5ff7-0d63-eeb1d8ed8cef', -1),
('5ada22d3-3248-6596-6e1a-bb545470e80c', '34cde64f-62e4-186c-77dd-867949feb8b5', -1),
('15f0f2b5-5e6d-6d8a-df78-cf6d4dd5c4a3', '5b2fcdf5-2050-7cb2-8f90-67245e8b283e', -1),
('78f9adfc-16b6-5ed3-f9c3-4d1f326d5ebd', '6a4d99cd-242a-6200-7add-867949feb8b5', -1),
('18bce813-54a9-6649-f8c3-4d1f326d5ebd', '2bf413d3-7a2c-217a-78dd-867949feb8b5', 1),
('18bce813-54a9-6649-f8c3-4d1f326d5ebd', '3c28ae33-ce6a-488c-8d53-e3978efa08ee', 1),
('37347d52-7eeb-3f66-ff4d-8601bf3d85d8', '5660d465-6020-48f3-75dd-867949feb8b5', -1),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '17120d02-6ab5-3e43-18cb-66948daf6128', -1),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '8acd0518-13ab-4556-8853-678921f51367', 1),
('145aebb1-a59e-4f22-93eb-cacd263eaf3f', '9e7f8a40-44bd-4692-9653-0cdcc1e68563', -1);

-- 
-- Dumping data for table questiontag
--
INSERT INTO questiontag VALUES
('7ad34860-1ac8-7ad7-d01d-086412a8ea97', '37dd9bb0-4c53-4134-31de-23e177779933'),
('5fab4ff7-1b8e-64d0-1880-de64d090a2c2', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('3738ce93-1661-5628-2cc2-579daad24557', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('5de704c2-573f-1a71-1a80-de64d090a2c2', '37dd9bb0-4c53-4134-31de-23e177779933'),
('506acf1e-5331-67a9-0d77-d9ebc53aac3d', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('1e5e96fa-5d7a-599d-27c2-579daad24557', '37dd9bb0-4c53-4134-31de-23e177779933'),
('6f51ea57-13b2-139f-1177-d9ebc53aac3d', '37dd9bb0-4c53-4134-31de-23e177779933'),
('7b666cc3-1b67-6037-67f6-423b4e874d0f', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('481183cb-5a6c-795b-68f6-423b4e874d0f', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('2afb2146-430d-394b-3d35-0028ed526dd0', '37dd9bb0-4c53-4134-31de-23e177779933'),
('3fed64ec-2f94-798f-0335-60b3ebf808a9', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('12e1b096-3542-127a-1077-d9ebc53aac3d', '37dd9bb0-4c53-4134-31de-23e177779933'),
('73dc79e1-1ccb-35a6-0235-60b3ebf808a9', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('6c80a684-1cb5-41af-ff34-60b3ebf808a9', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('56dce332-76f0-6b9b-5080-7ed9d1ea06ea', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('4db9a514-6d87-2ec1-3b35-0028ed526dd0', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('2356a2d8-7751-6f97-66f6-423b4e874d0f', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('42b9446a-790b-3892-2ac2-579daad24557', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('3e51a46c-4153-229c-7f41-c1ec321fe728', '37dd9bb0-4c53-4134-31de-23e177779933'),
('12693b29-274c-35ce-d11d-086412a8ea97', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('409dcd1a-426a-23f7-3a35-0028ed526dd0', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('36ac2df8-2b1e-368f-cc1d-086412a8ea97', '3b86d2ed-446c-5fce-56be-406293204378'),
('5254e49d-4368-3162-29c2-579daad24557', '37dd9bb0-4c53-4134-31de-23e177779933'),
('677132b4-2a33-68d1-26c2-579daad24557', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('11a8748f-3464-740c-28c2-579daad24557', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('7bd9cf39-3f55-3a7b-0035-60b3ebf808a9', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('595e7d51-318c-39e8-1377-d9ebc53aac3d', '3b86d2ed-446c-5fce-56be-406293204378'),
('5a841b92-791c-78df-5180-7ed9d1ea06ea', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('4989ff24-23e7-7b82-b6d2-89b22d10517e', '3b86d2ed-446c-5fce-56be-406293204378'),
('5f03d745-7702-1c85-2bc2-579daad24557', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('197fda97-5a35-781d-0e77-d9ebc53aac3d', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('3304dddb-1b72-607f-25c2-579daad24557', '3b86d2ed-446c-5fce-56be-406293204378'),
('3e7dd0ec-7ee5-392a-1277-d9ebc53aac3d', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('6b550b8f-7ff9-51bc-0f77-d9ebc53aac3d', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('28cd1599-63bf-785f-4f80-7ed9d1ea06ea', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('57cdf8c4-47e3-5560-7e41-c1ec321fe728', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('6485b64f-7835-65a4-8041-c1ec321fe728', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('1f85f165-3ed3-370a-b7d2-89b22d10517e', '37dd9bb0-4c53-4134-31de-23e177779933'),
('5c7f0ec4-77f2-3794-cf1d-086412a8ea97', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('6a763a51-4322-5c5a-b5d2-89b22d10517e', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('6c5b9fe7-14c9-3bb5-b4d2-89b22d10517e', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('524751f2-1577-523d-ce1d-086412a8ea97', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('62563348-7bd5-6519-3c35-0028ed526dd0', '37dd9bb0-4c53-4134-31de-23e177779933'),
('2f43fc9d-1f7a-25fc-5280-7ed9d1ea06ea', '6b47b37f-3123-3ce7-14cf-9712082ff6cb'),
('62af2231-78a7-7caa-cd1d-086412a8ea97', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('56aaf263-3355-23ea-b8d2-89b22d10517e', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('18baaac1-2b18-2317-b9d2-89b22d10517e', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('407db4d1-222b-287d-0135-60b3ebf808a9', '3b86d2ed-446c-5fce-56be-406293204378'),
('13ede5b6-19dd-563e-1980-de64d090a2c2', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('b2f86f84-7d9d-48aa-954e-879a723d3732', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('cadda7e7-cd37-4add-8699-6808e258d60e', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('cadda7e7-cd37-4add-8699-6808e258d60e', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('042272c3-cfff-4dab-81ef-e90b29f3e52a', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('042272c3-cfff-4dab-81ef-e90b29f3e52a', '3b86d2ed-446c-5fce-56be-406293204378'),
('40a13723-64bf-409c-a436-6125ec27e2f1', '37dd9bb0-4c53-4134-31de-23e177779933'),
('40a13723-64bf-409c-a436-6125ec27e2f1', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('35c3a053-254d-33f3-1b80-de64d090a2c2', '25c6c36e-1668-7d10-6e09-bf1378b8dc91'),
('35c3a053-254d-33f3-1b80-de64d090a2c2', '3700cc49-55b5-69ea-4929-a2925c0f334d'),
('ac809355-11d4-4ea4-8ff6-1de0385f9bdb', '148ed882-32b8-218e-9c20-39c2f00615e8'),
('ac809355-11d4-4ea4-8ff6-1de0385f9bdb', '354f1b13-17bf-1b52-87d5-ba100c6f7bce'),
('ac809355-11d4-4ea4-8ff6-1de0385f9bdb', '3b86d2ed-446c-5fce-56be-406293204378'),
('52f3ff99-7fff-436f-9404-c27ed4e2a482', '1b691e79-236d-5b5a-9d20-39c2f00615e8'),
('52f3ff99-7fff-436f-9404-c27ed4e2a482', '354f1b13-17bf-1b52-87d5-ba100c6f7bce');

-- 
-- Restore previous SQL mode
-- 
/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;

-- 
-- Enable foreign keys
-- 
/*!40014 SET FOREIGN_KEY_CHECKS = @OLD_FOREIGN_KEY_CHECKS */;